{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 问题：预测航班延误\n",
    "\n",
    "本笔记本的多个目标为：\n",
    "- 处理下载的 ZIP 文件并利用其创建数据集\n",
    "- 执行探索性数据分析 (EDA)\n",
    "- 建立基准模型\n",
    "- 将简单模型转化为集成模型\n",
    "- 进行超参数优化\n",
    "- 确认特征重要性\n",
    "\n",
    "## 业务场景简介\n",
    "您在一家行程预订网站工作，该网站致力于改善航班延误的旅客的客户体验。该公司希望开发一种功能，让美国的乘客在预订国内最繁忙机场的到港或离港航班时，能够知道航班是否会因天气原因而延误。\n",
    "\n",
    "您的任务是利用机器学习来确定航班是否会因天气原因而延误，从而在一定程度上解决这一问题。您可以访问大型航空公司国内航班的准点率数据集。您可以使用这一数据来训练机器学习模型，以便预测最繁忙机场的航班是否会延误。\n",
    "\n",
    "## 关于该数据集\n",
    "该数据集包含经过认证的美国航空公司报告的预定和实际的出发与到达时间。对于预定行程的国内旅客产生的收入，这些航空公司至少占据了 1%。数据由美国交通统计局 (BTS) 下属的航空公司信息办公室收集。数据集包含 2013 到 2018 年间航班的日期、时刻、出发地、目的地、航空公司、航程以及航班延误状态。\n",
    "\n",
    "### 特征\n",
    "有关数据集内的特征的更多信息，请参阅 [准点延误数据集特征](https://www.transtats.bts.gov/Fields.asp)。\n",
    "\n",
    "### 数据集属性  \n",
    "网站：https://www.transtats.bts.gov/\n",
    "\n",
    "本实验使用的数据集由美国交通统计局 (BTS) 下属的航空公司信息办公室编制，取自航空公司准点率数据库，网站为 https://www.transtats.bts.gov/DatabaseInfo.asp?DB_ID=120&amp;DB_URL=Mode_ID=1&amp;Mode_Desc=Aviation&amp;Subject_ID2=0。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 步骤 1：问题定义和数据收集\n",
    "\n",
    "在开始这个项目时，先在下面用几句话来总结在这个场景中要解决的业务问题和要实现的业务目标。同时说明您想让自己的团队衡量的业务指标。确定这些信息后，请清楚地写出机器学习问题陈述。最后，添加一两条备注，说明问题陈述所对应的机器学习的类型。\n",
    "\n",
    "#### <span style=\"color: blue;\">项目演示：在项目演示中总结这些详细信息。</span>\n",
    "\n",
    "### 1. 确定 ML 是否是适合部署的解决方案以及原因。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在此处写下答案"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 定义业务问题、成功指标和期望的 ML 输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在此处写下答案"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 确定您要解决的 ML 问题的类型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在此处写下答案"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 分析您要使用的数据是否合适。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在此处写下答案"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 设置\n",
    "\n",
    "我们已经确定了工作重点，现在我们来进行一些设置，以便开始解决问题。\n",
    "\n",
    "**注意：**该笔记本是在 `ml.m4.xlarge` 笔记本实例上创建和测试的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib2 import Path\n",
    "from zipfile import ZipFile\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import subprocess\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 步骤 2：数据预处理和可视化  \n",
    "在数据预处理阶段，您应该利用机会探索数据并将其可视化，以便更好地了解数据。首先，导入必要的库并将数据读入 Pandas dataframe。然后，探索数据。查看数据集的形状，探索要使用的列及其类型（数值型、分类型）。对各项特征执行基本统计，以便了解特征的均值和范围；深入分析目标列并确定其分布。\n",
    "\n",
    "### 要考虑的具体问题\n",
    "1. 您可以从对特征执行的基本统计中推断出什么？ \n",
    "\n",
    "2. 您可以从目标类的分布中推断出什么？\n",
    "\n",
    "3. 您从数据探索中还推断出了什么？\n",
    "\n",
    "#### <span style=\"color: blue;\">项目演示：在项目演示中总结这些问题和其他类似问题的答案。</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先将 Amazon S3 公有存储桶中的数据集引入这个笔记本环境。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查文件是否已位于所需路径中，或者是否需要下载\n",
    "\n",
    "base_path = '/home/ec2-user/SageMaker/project/data/FlightDelays/'\n",
    "csv_base_path = '/home/ec2-user/SageMaker/project/data/csvFlightDelays/'\n",
    "file_path = 'On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2014_1.zip'\n",
    "\n",
    "if not os.path.isfile(base_path + file_path):\n",
    "    subprocess.run(['mkdir', '-p', base_path])\n",
    "    subprocess.run(['mkdir', '-p', csv_base_path])\n",
    "    subprocess.run(['aws', 's3', 'cp',\n",
    "                    's3://aws-tc-largeobjects/ILT-TF-200-MLDWTS/flight_delay_project/csvFlightData-5/',\n",
    "                    base_path,'--recursive'])\n",
    "else:\n",
    "    print('File already downloaded!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_files = [str(file) for file in list(Path(base_path).iterdir()) if '.zip' in str(file)]\n",
    "len(zip_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 从 ZIP 文件中提取 CSV 文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def zip2csv(zipFile_name , file_path = '/home/ec2-user/SageMaker/project/data/csvFlightDelays'):\n",
    "    \"\"\"\n",
    "    从 zip 文件中提取 csv 文件\n",
    "    zipFile_name：zip 文件的名称\n",
    "    file_path : 用于存储 csv 文件的文件夹名称\n",
    "    \"\"\"\n",
    "    fname='On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2014_1.csv'\n",
    "    try:\n",
    "        if not os.path.isfile(csv_base_path + fname):\n",
    "            with ZipFile(zipFile_name, 'r') as z: \n",
    "                print(f'Extracting {zipFile_name} ') \n",
    "                z.extractall(path=file_path) \n",
    "    except:\n",
    "        print(f'zip2csv failed for {zipFile_name}')\n",
    "\n",
    "for file in zip_files:\n",
    "    zip2csv(file)\n",
    "\n",
    "print(\"Files Extracted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_files = [str(file) for file in list(Path(csv_base_path).iterdir()) if '.csv' in str(file)]\n",
    "len(csv_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在加载 CSV 文件前，请先阅读提取的文件夹中的 HTML 文件。该 HTML 文件介绍了相关背景以及有关数据集内包含的特征的更多信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame, HTML\n",
    "\n",
    "IFrame(src=\"./data/csvFlightDelays/readme.html\", width=1000, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 加载示例 CSV\n",
    "\n",
    "在合并所有 CSV 文件前，需要了解各个 CSV 文件中的数据。先使用 Pandas 读取 `On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2018_9.csv` 文件。您可以使用 Python 内置的 `read_csv` 函数（[文档](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html)）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = pd.read_csv(<CODE> # **在此输入代码**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**问题**：打印数据集内的行数和列数，并打印列名。\n",
    "\n",
    "**提示**：使用 `<dataframe>.shape` 函数可以查看 DataFrame 的行和列，使用 `<dataframe>.columns` 函数可以查看列名。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shape = # **在此输入代码**\n",
    "print(f'Rows and columns in one csv file is {df_shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**问题**：打印数据集的前 10 行。 \n",
    "\n",
    "**提示**：使用内置的 Pandas 函数 `head(x)` 可以打印前 `x` 行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 在此处输入代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**问题**：打印数据集里的所有列。使用 `<dataframe>.columns` 可以查看列名。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The column names are :')\n",
    "print('#########')\n",
    "for col in <CODE>:# **在此处输入代码**\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**问题**：打印数据集里包含 'Del' 一词的所有列。这可以让您了解包含延误数据的列有多少。\n",
    "\n",
    "**提示**：您可以使用 Python 的列表推导功能来列出符合特定 `if` 语句条件的值。\n",
    "\n",
    "例如：`[x for x in [1,2,3,4,5] if x > 2]`  \n",
    "\n",
    "**提示**：您可以使用 `in` 关键字（[文档](https://www.w3schools.com/python/ref_keyword_in.asp)）来确认值是否位于列表中。\n",
    "\n",
    "例如：`5 in [1,2,3,4,5]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在此处输入代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下列问题也可以帮助您了解有关数据集的更多信息。\n",
    "\n",
    "**问题**   \n",
    "1. 数据集有多少个行和列？   \n",
    "2. 数据集里包含多少个年份？   \n",
    "3. 数据集的日期范围是多少？   \n",
    "4.数据集里包含哪些航空公司？   \n",
    "5.数据集里包含哪些出发地和目的地机场？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The #rows and #columns are \", <CODE> , \" and \", <CODE>)\n",
    "print(\"The years in this dataset are: \", <CODE>)\n",
    "print(\"The months covered in this dataset are: \", <CODE>)\n",
    "print(\"The date range for data is :\" , min(<CODE>), \" to \", max(<CODE>))\n",
    "print(\"The airlines covered in this dataset are: \", list(<CODE>))\n",
    "print(\"The Origin airports covered are: \", list(<CODE>))\n",
    "print(\"The Destination airports covered are: \", list(<CODE>))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**问题**：出发地和目的地机场的总数分别是多少？\n",
    "\n",
    "**提示**：您可以使用 Pandas 函数 `values_count`（[文档](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.value_counts.html)），通过 `Origin` 列和 `Dest` 列来确定每个机场对应的值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = pd.DataFrame({'Origin':<CODE>, 'Destination':<CODE>})\n",
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**问题**：根据数据集里的航班数量，打印出前 15 个出发地和目的地机场。\n",
    "\n",
    "**提示**：您可以使用 Pandas 函数 `sort_values`（[文档](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sort_values.html)）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts.sort_values(by=<CODE>,ascending=False).head(15 )# 在此处输入代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**问题**：掌握了有关某个航班的所有信息后，您能否预测其是否会延误？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在此处输入答案："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设您要从旧金山到洛杉矶出差。您想要通过一组特征来预测航班是否会延误，以便更好地预订洛杉矶的酒店。在登机之前，您会在这个数据集里了解多少特征？\n",
    "\n",
    "`DepDelay`、`ArrDelay`、`CarrierDelay`、`WeatherDelay`、`NASDelay`、`SecurityDelay`、`LateAircraftDelay` 和 `DivArrDelay` 等列包含有关延误的信息。但是，这种延误可能发生在出发地，也可能发生在目的地。如果航班因天气骤变而导致延误了 10 分钟才到达，则这种数据对您预订洛杉矶的酒店没有帮助。\n",
    "\n",
    "因此，要简化问题陈述，请考虑用以下各列来预测到达延误：<br>\n",
    "\n",
    "`Year`、`Quarter`、`Month`、`DayofMonth`、`DayOfWeek`、`FlightDate`、`Reporting_Airline`、`Origin`、`OriginState`、`Dest`、`DestState`、`CRSDepTime`、`DepDelayMinutes`、`DepartureDelayGroups`、`Cancelled`、`Diverted`、`Distance`、`DistanceGroup`、`ArrDelay`、`ArrDelayMinutes`、`ArrDel15` 和 `AirTime`\n",
    "\n",
    "您也可以将出发地和目的地机场筛选为：\n",
    "- 热门机场：ATL、ORD、DFW、DEN、CLT、LAX、IAH、PHX、SFO\n",
    "- 前 5 大航空公司：UA、OO、WN、AA、DL\n",
    "\n",
    "这有助于减少要合并的 CSV 文件中的数据大小。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 合并所有 CSV 文件\n",
    "\n",
    "**提示**：  \n",
    "首先，创建一个空 DataFrame，用于复制每个文件中的 DataFrame。然后，针对 `csv_files` 列表中的每个文件执行以下操作：\n",
    "\n",
    "1. 将 CSV 文件读入 DataFrame  \n",
    "2. 根据 `filter_cols` 变量来筛选列\n",
    "\n",
    "```\n",
    "        columns = ['col1', 'col2']\n",
    "        df_filter = df[columns]\n",
    "```\n",
    "\n",
    "3. 仅保留每个 subset_cols 中的 subset_vals。使用 Pandas 函数 `isin`（[文档](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.isin.html)）来确认 `val` 是否位于 DataFrame 列中，然后选择包含它的行。\n",
    "\n",
    "```\n",
    "        df_eg[df_eg['col1'].isin('5')]\n",
    "```\n",
    "\n",
    "4. 将该 DataFrame 与空 DataFrame 合并 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_csv(csv_files, filter_cols, subset_cols, subset_vals, file_name = 'data/combined_files.csv'):\n",
    "    \"\"\"\n",
    "    将 csv 文件合并到一个 DataFrame 中\n",
    "    csv_files：csv 文件路径的列表\n",
    "    filter_cols：要筛选的列的列表\n",
    "    subset_cols：包含子集行的列的列表\n",
    "    subset_vals：包含子集行的值的列表\n",
    "    \"\"\"\n",
    "    # 创建一个空 DataFrame\n",
    "    df = # 在此处输入代码 \n",
    "    \n",
    "    for file in csv_files:\n",
    "        # 将 CSV 文件读入 DataFrame\n",
    "        df_temp = pd.read_csv(<CODE>)# 在此输入代码\n",
    "        \n",
    "        # 根据 filter_cols 变量来筛选列\n",
    "        # 例如 columns = ['col1', 'col2']\n",
    "        # df_filter = df[columns]\n",
    "        df_temp = # 在此处输入代码\n",
    "        \n",
    "        # 仅保留每个 subset_cols 中的 subset_vals\n",
    "        ＃ 提示：使用 `isin` 函数来确认 val 是否位于 DataFrame 列中\n",
    "        # 然后选择包含它的行\n",
    "        # 例如 df[df['col1'].isin('5')]\n",
    "        for col, val in zip(subset_cols,subset_vals):\n",
    "            df_temp = # 在此处输入代码     \n",
    "        \n",
    "        ＃ 使用 Pandas 连结 `pd.concat`，以连结主 DataFrame 与每个文件的 DataFrame\n",
    "        df = pd.concat([df, df_temp], axis=0)\n",
    "    \n",
    "        \n",
    "    df.to_csv(file_name, index=False)\n",
    "    print(f'Combined csv stored at {file_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cols 是用于预测到达延误的列的列表 \n",
    "cols = ['Year','Quarter','Month','DayofMonth','DayOfWeek','FlightDate',\n",
    "        'Reporting_Airline','Origin','OriginState','Dest','DestState',\n",
    "        'CRSDepTime','Cancelled','Diverted','Distance','DistanceGroup',\n",
    "        'ArrDelay','ArrDelayMinutes','ArrDel15','AirTime']\n",
    "\n",
    "subset_cols = ['Origin', 'Dest', 'Reporting_Airline']\n",
    "\n",
    "# subset_vals 是热门出发地和目的地以及前 5 大航空公司的列表集合\n",
    "subset_vals = [['ATL', 'ORD', 'DFW', 'DEN', 'CLT', 'LAX', 'IAH', 'PHX', 'SFO'],\n",
    "               ['ATL', 'ORD', 'DFW', 'DEN', 'CLT', 'LAX', 'IAH', 'PHX', 'SFO'],\n",
    "               ['UA', 'OO', 'WN', 'AA', 'DL']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用上述函数将所有不同文件合并到一个可以轻松读取的文件中。\n",
    "\n",
    "**注意**：这一过程需要 5 到 7 分钟才能完成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "combine_csv(csv_files, cols, subset_cols, subset_vals)\n",
    "print(f'csv\\'s merged in {round((time.time() - start)/60,2)} minutes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 加载数据集\n",
    "\n",
    "加载已合并的数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(<CODE>)# 在此处输入代码，以阅读合并的 csv 文件。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "打印前 5 个记录。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在此处输入代码 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下列问题也可以帮助您了解有关数据集的更多信息。\n",
    "\n",
    "**问题**   \n",
    "1. 数据集有多少个行和列？   \n",
    "2. 数据集里包含多少个年份？   \n",
    "3. 数据集的日期范围是多少？   \n",
    "4. 数据集里包含哪些航空公司？   \n",
    "5.数据集里包含哪些出发地和目的地机场？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The #rows and #columns are \", <CODE> , \" and \", <CODE>)\n",
    "print(\"The years in this dataset are: \", list(<CODE>))\n",
    "print(\"The months covered in this dataset are: \", sorted(list(<CODE>)))\n",
    "print(\"The date range for data is :\" , min(<CODE>), \" to \", max(<CODE>))\n",
    "print(\"The airlines covered in this dataset are: \", list(<CODE>))\n",
    "print(\"The Origin airports covered are: \", list(<CODE>))\n",
    "print(\"The Destination airports covered are: \", list(<CODE>))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们来定义**目标列：is_delay**（航班到达延误超过 15 分钟时，其值为 1，否则为 0）。使用 `rename` 方法将 `ArrDel15` 列重命名为 `is_delay`。\n",
    "\n",
    "**提示**：您可以使用 Pandas 函数 `rename`（[文档](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.rename.html)）。\n",
    "\n",
    "例如：\n",
    "```\n",
    "df.rename(columns={'col1':'column1'}, inplace=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rename(columns=<CODE>, inplace=True) # 在此处输入代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查找各列的空值。您可以使用 `isnull()` 函数（[文档](https://pandas.pydata.org/pandas-docs/version/0.17.0/generated/pandas.isnull.html)）。\n",
    "\n",
    "**提示**：`isnull()` 可以检测出特定值是否为空值，并在其位置给出布尔值（True 或 False）。使用 `sum(axis=0)` 函数可以计算列的总数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在此处输入代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在 1658130 个行中，有 22540 个行缺少航班延误详情和飞行时间，占 1.3%。您可以删除或替换这些行。文档没有提到有关缺少信息的行的任何信息。\n",
    "\n",
    "**提示**：使用 `~` 运算符可以从 `isnull()` 的输出选择非空值。\n",
    "\n",
    "例如：\n",
    "```\n",
    "null_eg = df_eg[~df_eg['column_name'].isnull()]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 删除空值列\n",
    "data = # 在此处输入代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将 CRSDepTime 转化为 24 小时制时间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['DepHourofDay'] = # 在此处输入代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **ML 问题陈述**\n",
    "- 掌握了一组特征后，您能否预测出某个航班是否会延误 15 分钟以上？\n",
    "- 目标变量只会有 0 和 1 两个值，因此您可以使用分类算法。\n",
    "\n",
    "在开始建模前，最好先查看特征的分布和相关性等信息。\n",
    "- 这可以让您了解数据中的任何非线性/模式。\n",
    "    - 线性模型：添加幂/指数/交互特征\n",
    "    - 尝试非线性模型\n",
    "- 数据不平衡 \n",
    "    - 选择反映的模型效果不会有偏差的指标（准确率与 AUC）\n",
    "    - 使用加权/自定义损失函数\n",
    "- 缺少数据\n",
    "    - 基于简单的统计数据进行替换：均值、中位数、众数（数值变量）和频繁类（分类变量）\n",
    "    - 基于集群进行替换（用 KNN 预测列值）\n",
    "    - 删除列"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据探索\n",
    "\n",
    "#### 查看延误类和不延误类\n",
    "\n",
    "**提示**：使用 `groupby` 图（[文档](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html)）和 `bar` 图（[文档](https://matplotlib.org/tutorials/introductory/pyplot.html)）来绘制频率和类的分布。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(data.groupby(<CODE>).size()/len(data) ).plot(kind='bar')# 在此处输入代码\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of classes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**问题**：您能从柱状图中推断出有关延误率和不延误率的什么信息？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在此处输入答案："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**问题**： \n",
    "\n",
    "- 哪个月的延误次数最多？\n",
    "- 一天之中什么时候的延误次数最多？\n",
    "- 星期几的延误次数最多？\n",
    "- 哪家航空公司的延误次数最多？\n",
    "- 哪个出发地和目的地机场的延误次数最多？\n",
    "- 航程是不是与延误有关的一项因素？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_columns = ['Month', 'DepHourofDay', 'DayOfWeek', 'Reporting_Airline', 'Origin', 'Dest']\n",
    "fig, axes = plt.subplots(3, 2, figsize=(20,20), squeeze=False)\n",
    "# fig.autofmt_xdate(rotation=90)\n",
    "\n",
    "for idx, column in enumerate(viz_columns):\n",
    "    ax = axes[idx//2, idx%2]\n",
    "    temp = data.groupby(column)['is_delay'].value_counts(normalize=True).rename('percentage').\\\n",
    "    mul(100).reset_index().sort_values(column)\n",
    "    sns.barplot(x=column, y=\"percentage\", hue=\"is_delay\", data=temp, ax=ax)\n",
    "    plt.ylabel('% delay/no-delay')\n",
    "    \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot( x=\"is_delay\", y=\"Distance\", data=data, fit_reg=False, hue='is_delay', legend=False)\n",
    "plt.legend(loc='center')\n",
    "plt.xlabel('is_delay')\n",
    "plt.ylabel('Distance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在此处输入答案"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特征\n",
    "\n",
    "查看所有列及其具体类型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "筛选出所需的列：\n",
    "- Date 是多余的数据，因为已经有 Year、Quarter、Month、DayofMonth 和 DayOfWeek 可以描述日期。\n",
    "- 使用 Origin 和 Dest 代码，而不是 OriginState 和 DestState。\n",
    "- 因为您只是在对航班是否延误进行分类，所以不需要 TotalDelayMinutes、DepDelayMinutes 和 ArrDelayMinutes。\n",
    "\n",
    "将 DepHourofDay 视为分类变量，因为它与目标没有数量关系。\n",
    "- 如果必须对其进行独热编码，则会多出 23 列。\n",
    "- 处理分类变量的其他方法包括进行哈希编码、进行正则化均值编码以及将值拆分到存储桶中等等。\n",
    "- 在这里只需拆分到存储桶中即可。\n",
    "\n",
    "**提示**：要将列类型更改为类别，请使用 `astype` 函数（[文档](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.astype.html)）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_orig = data.copy()\n",
    "data = data[[ 'Quarter', 'Month', 'DayofMonth', 'DayOfWeek',\n",
    "       'Reporting_Airline', 'Origin', 'Dest','Distance','DepHourofDay', 'is_delay']]\n",
    "categorical_columns = ['Quarter', 'Month', 'DayofMonth', 'DayOfWeek',\n",
    "       'Reporting_Airline', 'Origin', 'Dest', 'DepHourofDay', 'is_delay']\n",
    "for c in categorical_columns:\n",
    "    data[c] = data[c].astype('category')# 在此处输入代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要使用独热编码，请针对上述选择的分类数据列使用 Pandas 函数 `get_dummies`。然后可以使用 Pandas 函数 `concat` 将生成的这些特征合并到原始数据集中。要对分类变量进行编码，您也可以使用关键字 `drop_first=True` 来进行*虚拟编码*。有关虚拟编码的更多信息，请参阅 https://en.wikiversity.org/wiki/Dummy_variable_(statistics)。\n",
    "\n",
    "例如：\n",
    "```\n",
    "pd.get_dummies(df[['column1','columns2']], drop_first=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dummies = pd.get_dummies(<CODE>, drop_first=True) # 在此处输入代码\n",
    "data = pd.concat([<CODE>, <CODE>], axis = 1)\n",
    "categorical_columns.remove('is_delay')\n",
    "data.drop(categorical_columns,axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "确认数据集和新列的长度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在此处输入代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在此处输入代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**示例答案：** \n",
    "```\n",
    "Index(['Distance', 'is_delay', 'Quarter_2', 'Quarter_3', 'Quarter_4',\n",
    "       'Month_2', 'Month_3', 'Month_4', 'Month_5', 'Month_6', 'Month_7',\n",
    "       'Month_8', 'Month_9', 'Month_10', 'Month_11', 'Month_12',\n",
    "       'DayofMonth_2', 'DayofMonth_3', 'DayofMonth_4', 'DayofMonth_5',\n",
    "       'DayofMonth_6', 'DayofMonth_7', 'DayofMonth_8', 'DayofMonth_9',\n",
    "       'DayofMonth_10', 'DayofMonth_11', 'DayofMonth_12', 'DayofMonth_13',\n",
    "       'DayofMonth_14', 'DayofMonth_15', 'DayofMonth_16', 'DayofMonth_17',\n",
    "       'DayofMonth_18', 'DayofMonth_19', 'DayofMonth_20', 'DayofMonth_21',\n",
    "       'DayofMonth_22', 'DayofMonth_23', 'DayofMonth_24', 'DayofMonth_25',\n",
    "       'DayofMonth_26', 'DayofMonth_27', 'DayofMonth_28', 'DayofMonth_29',\n",
    "       'DayofMonth_30', 'DayofMonth_31', 'DayOfWeek_2', 'DayOfWeek_3',\n",
    "       'DayOfWeek_4', 'DayOfWeek_5', 'DayOfWeek_6', 'DayOfWeek_7',\n",
    "       'Reporting_Airline_DL', 'Reporting_Airline_OO', 'Reporting_Airline_UA',\n",
    "       'Reporting_Airline_WN', 'Origin_CLT', 'Origin_DEN', 'Origin_DFW',\n",
    "       'Origin_IAH', 'Origin_LAX', 'Origin_ORD', 'Origin_PHX', 'Origin_SFO',\n",
    "       'Dest_CLT', 'Dest_DEN', 'Dest_DFW', 'Dest_IAH', 'Dest_LAX', 'Dest_ORD',\n",
    "       'Dest_PHX', 'Dest_SFO'],\n",
    "      dtype='object')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在可以开始训练模型了。在拆分数据前，请将 `is_delay` 列重命名为 `target`。\n",
    "\n",
    "**提示**：您可以使用 Pandas 函数 `rename`（[文档](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.rename.html)）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rename(columns = {<CODE>:<CODE>}, inplace=True )# 在此处输入代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">实验 2 结束</span>\n",
    "\n",
    "将项目文件保存到本地计算机。按照以下步骤进行操作：\n",
    "\n",
    "1. 在页面顶部，单击 **File（文件）**菜单。\n",
    "\n",
    "1. 选择 **Download as（另存为）**，然后单击 **Notebook (.ipynb)**。 \n",
    "\n",
    "这会将当前的笔记本下载到计算机上默认的下载文件夹中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 步骤 3：模型训练和评估\n",
    "\n",
    "在将数据集从 dataframe 转换为可供机器学习算法使用的格式之前，您必须完成一些预备步骤。对于 Amazon SageMaker，您需要采取以下步骤：\n",
    "\n",
    "1. 使用 `sklearn.model_selection.train_test_split` 将数据拆分为 `train_data`、`validation_data` 和 `test_data`。 \n",
    "2. 将数据集转换为适合 Amazon SageMaker 训练作业使用的文件格式。可以是 CSV 文件或记录 protobuf。有关更多信息，请参阅 [适用于训练的常见数据格式](https://docs.aws.amazon.com/sagemaker/latest/dg/cdf-training.html)。 \n",
    "3. 将数据上传到 Amazon S3 存储桶中。如果您从未创建过存储桶，请参阅 [创建存储桶](https://docs.aws.amazon.com/AmazonS3/latest/gsg/CreatingABucket.html)。 \n",
    "\n",
    "使用以下单元格完成这些步骤。根据需要插入和删除单元格。\n",
    "\n",
    "#### <span style=\"color: blue;\">项目陈述：请在项目陈述中记下您在这一阶段做出的关键决定。</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练测试拆分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def create_training_sets(data):\n",
    "    \"\"\"\n",
    "    将 DataFrame 转换为训练、验证和测试\n",
    "    params:\n",
    "        data：要拆分数据集的 DataFrame\n",
    "    返回值：\n",
    "        train_features：训练特征数据集\n",
    "        test_features：测试特征数据集 \n",
    "        train_labels：训练数据集的标签\n",
    "        test_labels：测试数据集的标签\n",
    "        val_features：验证特征数据集\n",
    "        val_labels：验证数据集的标签\n",
    "    \"\"\"\n",
    "    # 从 DataFrame 提取目标变量并将类型转换为 float32\n",
    "    ys = np.array(<CODE>).astype(\"float32\") # 在此处输入代码\n",
    "    \n",
    "    # 删除所有不需要的列，包括目标列\n",
    "    drop_list = # 在此处输入代码\n",
    "    \n",
    "    # 删除 drop_list 中的列并将数据转换为 float32 类型的 NumPy 数组\n",
    "    xs = np.array(data.drop(<CODE>, axis=1)).astype(\"float32\")# 在此处输入代码\n",
    "    \n",
    "    np.random.seed(0)\n",
    "\n",
    "    # 使用 sklearn 函数 train_test_split 按训练 80% 和测试 20% 的比例拆分数据集\n",
    "    # 示例：train_test_split(x, y, test_size=0.3)\n",
    "    train_features, test_features, train_labels, test_labels = # 在此处输入代码\n",
    "    \n",
    "    # 再次使用 sklearn 函数将测试数据集拆分为 50% 用作验证和 50% 用作测试\n",
    "    val_features, test_features, val_labels, test_labels = # 在此处输入代码\n",
    "    \n",
    "    \n",
    "    return train_features, test_features, train_labels, test_labels, val_features, val_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用函数来创建您的数据集\n",
    "train_features, test_features, train_labels, test_labels, val_features, val_labels = create_training_sets(data)\n",
    "\n",
    "print(f\"Length of train_features is: {<CODE>}\")\n",
    "print(f\"Length of train_labels is: {<CODE>}\")\n",
    "print(f\"Length of val_features is: {<CODE>}\")\n",
    "print(f\"Length of val_labels is: {<CODE>}\")\n",
    "print(f\"Length of test_features is: {<CODE>}\")\n",
    "print(f\"Length of test_labels is: {<CODE>}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**示例答案**\n",
    "```\n",
    "train_features 的行列数为：(1308472, 71)\n",
    "train_labels 的行列数为：(1308472,)\n",
    "val_features 的行列数为：(163559, 71)\n",
    "val_labels 的行列数为：(163559,)\n",
    "test_features 的行列数为：(163559, 71)\n",
    "test_labels 的行列数为：(163559,)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基准分类模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.predictor import csv_serializer\n",
    "from sagemaker.amazon.amazon_estimator import RecordSet\n",
    "import boto3\n",
    "\n",
    "num_classes = # 在此处输入代码\n",
    "\n",
    "# 实例化线性学习器评估程序对象\n",
    "classifier_estimator = sagemaker.LinearLearner(role=sagemaker.get_execution_role(),\n",
    "                                               train_instance_count=<CODE>,\n",
    "                                               train_instance_type=<CODE>,\n",
    "                                               predictor_type=<CODE>,\n",
    "                                              binary_classifier_model_selection_criteria=<CODE>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 示例代码\n",
    "```\n",
    "num_classes = len(pd.unique(train_labels))\n",
    "classifier_estimator = sagemaker.LinearLearner(role=sagemaker.get_execution_role(),\n",
    "                                              train_instance_count=1,\n",
    "                                              train_instance_type='ml.m4.xlarge',\n",
    "                                              predictor_type='binary_classifier',\n",
    "                                             binary_classifier_model_selection_criteria = 'cross_entropy_loss')\n",
    "                                              \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "线性学习器接受内容类型为 protobuf 或 CSV 的训练数据以及内容类型为 protobuf、CSV 或 JSON 的推理请求。训练数据有特征和 ground-truth 标签，而推理请求中的数据只有特征。\n",
    "\n",
    "在生产管道中，建议将数据转换为 Amazon SageMaker protobuf 格式并将其存储在 Amazon S3 中。但是，为了快速启动并运行，AWS 能够在数据集小到足以装入本地内存时提供便捷的 `record_set` 方法来进行转换和上传。该方法接受您已有的 NumPy 数组，所以我们在这里就用它。`RecordSet` 对象将会跟踪数据的临时 Amazon S3 位置。使用 `estimator.record_set` 函数来创建训练、验证和测试记录。然后使用 `estimator.fit` 函数启动训练作业。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 创建训练、验证和测试记录\n",
    "train_records = linear.record_set(<CODE>,<CODE>, channel='train')# 在此处输入代码\n",
    "val_records = linear.record_set(<CODE>,<CODE>, channel='validation')# 在此处输入代码\n",
    "test_records = linear.record_set(<CODE>,<CODE>, channel='test')# 在此处输入代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面，使用您上传的数据集训练模型。\n",
    "\n",
    "### 示例代码\n",
    "```\n",
    "linear.fit([train_records,val_records,test_records])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 安装分类器\n",
    "# 在此处输入代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型评估\n",
    "在本节中，您将评估自己训练的模型。首先，使用 `estimator.deploy` 函数（`initial_instance_count= 1` 且 `instance_type= 'ml.m4.xlarge'`）在 Amazon SageMaker 上部署模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 部署用于批量预测的终端节点\n",
    "classifier_predictor = classifier_estimator.deploy(initial_instance_count=<CODE>,\n",
    "                                                   instance_type=<CODE>) # 在此处输入代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当终端节点为 'InService' 时，请评估模型使用测试集的效果。请使用 `predict_batches` 函数利用测试集来预测指标。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "def predict_batches(predictor, features, labels):\n",
    "    \"\"\"\n",
    "    返回评估结果\n",
    "    predictor：模型的预测器对象\n",
    "    features：输入模型的特征\n",
    "    label：Ground Truth 目标值\n",
    "    \"\"\"\n",
    "    prediction_batches = [predictor.predict(batch) for batch in np.array_split(features, 100)]\n",
    "\n",
    "    # 解析 protobuf 响应来提取预测标签\n",
    "    extract_label = lambda x: x.label['predicted_label'].float32_tensor.values\n",
    "    preds = np.concatenate([np.array([extract_label(x) for x in batch]) for batch in prediction_batches])\n",
    "    preds = preds.reshape((-1,))\n",
    "\n",
    "    # 计算准确率\n",
    "    accuracy = (preds == labels).sum() / labels.shape[0]\n",
    "    print(f'Accuracy: {accuracy}')\n",
    "    \n",
    "    auc = roc_auc_score(labels, preds)\n",
    "    print(f'AUC : {auc}')\n",
    "    \n",
    "    precision, recall, f1_score, _ = precision_recall_fscore_support(labels, preds, average = 'binary')\n",
    "    print(f'Precision: {precision}')\n",
    "    print(f'Recall: {recall}')\n",
    "    print(f'F1_score: {f1_score}')\n",
    "    \n",
    "    confusion_matrix = pd.crosstab(index=labels, columns=np.round(preds), rownames=['True'], colnames=['predictions']).astype(int)\n",
    "    plt.figure(figsize = (5,5))\n",
    "    sns.heatmap(confusion_matrix, annot=True, fmt='.2f', cmap=\"YlGnBu\").set_title('Confusion Matrix') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要查看混淆矩阵，在测试数据集上运行 `predict_batches` 函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在此处输入代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 要考虑的关键问题：\n",
    "1. 您的模型在测试集上的表现与训练集上的表现相比如何？ 您可以从这种比较中推断出什么？ \n",
    "\n",
    "2. 准确率、查准率和查全率等指标结果之间有没有明显差异？ 如果是，为什么您会看到这些差异？ \n",
    "\n",
    "3. 考虑到您的业务状况和目标，您此时最需要考虑的指标是什么？ 为什么？\n",
    "\n",
    "4. 您认为最重要指标的结果是否足以满足您的业务需求？ 如果不能，那么您在下一次迭代中可能会更改哪些内容（在特征设计部分，下一步是什么）？ \n",
    "\n",
    "用下面的单元格来回答这些问题和其他问题。根据需要插入和删除单元格。\n",
    "\n",
    "#### <span style=\"color: blue;\">项目演示：在项目演示中记录这些问题的答案以及您可能会在本节中回答的其他类似问题的答案。在项目演示中记录关键的详细信息和您所做的关键决定。</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**问题**：您可以从混淆矩阵中总结出什么？\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在此处输入答案："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">实验 3 结束</span>\n",
    "\n",
    "将项目文件保存到本地计算机。按照以下步骤进行操作：\n",
    "\n",
    "1. 在页面顶部，单击 **File（文件）**菜单。\n",
    "\n",
    "1. 选择 **Download as（另存为）**，然后单击 **Notebook (.ipynb)**。 \n",
    "\n",
    "这会将当前的笔记本下载到计算机上默认的下载文件夹中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 迭代 II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 步骤 4：特征设计\n",
    "\n",
    "现在，您已经完成了一次训练和评估模型的迭代。鉴于模型第一次获得的结果可能不足以解决您的业务问题，您可以对数据进行哪些更改以改进模型的表现？\n",
    "\n",
    "### 要考虑的关键问题：\n",
    "1. 两个主要类（延误与不延误）的平衡可能会对模型效果有怎样的影响？\n",
    "2. 是否有任何相关联的特征？\n",
    "3. 在此阶段，您是否可以执行可能会对模型性能产生积极影响的特征约简技术？ \n",
    "4. 您能否想办法添加更多数据/数据集？\n",
    "4. 执行一些特征设计之后，模型效果与第一次迭代相比如何？\n",
    "\n",
    "使用下面的单元格执行您认为可以提高模型性能的特定特征设计技术（根据上述问题）。根据需要插入和删除单元格。\n",
    "\n",
    "#### <span style=\"color: blue;\">项目演示：在项目演示中记录您在本节中使用的主要决策和方法，以及在再次评估模型后获得的任何新性能指标。</span>\n",
    "\n",
    "在开始之前，思考一下为什么查准率和查全率在 80% 左右，而准确率为 99%。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 添加更多特征\n",
    "\n",
    "1. 节假日\n",
    "2. 天气"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2014 到 2018 年的节假日列表是已知数据，因此您可以创建指示变量 **is_holiday** 来标记这些节假日。\n",
    "我们假设节假日的航班延误比平时更多。请添加一个涵盖 2014 到 2018 年的节假日的布尔变量 `is_holiday`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 来源：http://www.calendarpedia.com/holidays/federal-holidays-2014.html\n",
    "\n",
    "holidays_14 = ['2014-01-01', '2014-01-20', '2014-02-17', '2014-05-26', '2014-07-04', '2014-09-01', '2014-10-13', '2014-11-11', '2014-11-27', '2014-12-25' ] \n",
    "holidays_15 = ['2015-01-01', '2015-01-19', '2015-02-16', '2015-05-25', '2015-06-03', '2015-07-04', '2015-09-07', '2015-10-12', '2015-11-11', '2015-11-26', '2015-12-25'] \n",
    "holidays_16 = ['2016-01-01', '2016-01-18', '2016-02-15', '2016-05-30', '2016-07-04', '2016-09-05', '2016-10-10', '2016-11-11', '2016-11-24', '2016-12-25', '2016-12-26']\n",
    "holidays_17 = ['2017-01-02', '2017-01-16', '2017-02-20', '2017-05-29' , '2017-07-04', '2017-09-04' ,'2017-10-09', '2017-11-10', '2017-11-23', '2017-12-25']\n",
    "holidays_18 = ['2018-01-01', '2018-01-15', '2018-02-19', '2018-05-28' , '2018-07-04', '2018-09-03' ,'2018-10-08', '2018-11-12','2018-11-22', '2018-12-25']\n",
    "# holidays_19 = ['2019-01-01', '2019-01-21', '2019-02-18', '2019-05-27' , '2019-07-04', '2019-09-02' ,'2019-10-14', '2019-11-11','2019-11-28', '2019-12-25']\n",
    "holidays = holidays_14+ holidays_15+ holidays_16 + holidays_17+ holidays_18\n",
    "\n",
    "### 为节假日添加指示变量\n",
    "data_orig['is_holiday'] = # 在此处输入代码 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "天气数据获取自 https://www.ncei.noaa.gov/access/services/data/v1?dataset=daily-summaries&amp;stations=USW00023174,USW00012960,USW00003017,USW00094846,USW00013874,USW00023234,USW00003927,USW00023183,USW00013881&amp;dataTypes=AWND,PRCP,SNOW,SNWD,TAVG,TMIN,TMAX&amp;startDate=2014-01-01&amp;endDate=2018-12-31。\n",
    "<br>\n",
    "\n",
    "该数据集包含多个城市的风速、降水量、降雪量和温度信息，按其机场代码列出。\n",
    "\n",
    "**问题**：雨、雪或大风造成的恶劣天气是否会导致航班延误？ 我们来确认一下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "aws s3 cp s3://aws-tc-largeobjects/ILT-TF-200-MLDWTS/flight_delay_project/daily-summaries.csv /home/ec2-user/SageMaker/project/data/\n",
    "#wget 'https://www.ncei.noaa.gov/access/services/data/v1?dataset=daily-summaries&amp;stations=USW00023174,USW00012960,USW00003017,USW00094846,USW00013874,USW00023234,USW00003927,USW00023183,USW00013881&amp;dataTypes=AWND,PRCP,SNOW,SNWD,TAVG,TMIN,TMAX&amp;startDate=2014-01-01&amp;endDate=2018-12-31' -O /home/ec2-user/SageMaker/project/data/daily-summaries.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将与机场代码对应的天气数据导入数据集中。使用下面的气象站和机场进行分析，并创建一个名为 `airport` 的新列，将气象站映射到机场名称。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = pd.read_csv(<CODE>) # 在此处输入代码，以读取daily-summaries.csv' 文件\n",
    "station = ['USW00023174','USW00012960','USW00003017','USW00094846',\n",
    "           'USW00013874','USW00023234','USW00003927','USW00023183','USW00013881'] \n",
    "airports = ['LAX', 'IAH', 'DEN', 'ORD', 'ATL', 'SFO', 'DFW', 'PHX', 'CLT']\n",
    "\n",
    "### 将气象站映射到机场代码\n",
    "station_map = # 在此处输入代码 \n",
    "weather['airport'] = # 在此处输入代码 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从 `DATE` 列中创建另一个名为 `MONTH` 的列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weather['MONTH'] = weather[<CODE>].apply(lambda x: x.split('-')[1])# 在此处输入代码 \n",
    "weather.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 示例输出\n",
    "```\n",
    "  STATION DATE AWND PRCP SNOW SNWD TAVG TMAX TMIN airport MONTH\n",
    "0 USW00023174 2014-01-01 16 0 NaN NaN 131.0 178.0 78.0 LAX 01\n",
    "1 USW00023174 2014-01-02 22 0 NaN NaN 159.0 256.0 100.0 LAX 01\n",
    "2 USW00023174 2014-01-03 17 0 NaN NaN 140.0 178.0 83.0 LAX 01\n",
    "3 USW00023174 2014-01-04 18 0 NaN NaN 136.0 183.0 100.0 LAX 01\n",
    "4 USW00023174 2014-01-05 18 0 NaN NaN 151.0 244.0 83.0 LAX 01\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用 `fillna()` 来分析并处理 `SNOW` 和 `SNWD` 列缺少的值。使用 `isna()` 函数来检查所有列缺少的值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weather.SNOW.fillna(<CODE>, inplace=True)# 在此处输入代码\n",
    "weather.SNWD.fillna(<CODE>, inplace=True)# 在此处输入代码\n",
    "weather.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**问题**：打印 TAVG、TMAX 和 TMIN 缺少值的行的索引。\n",
    "\n",
    "**提示**：使用 `isna()` 函数找出缺少的行，然后使用 idx 变量上的列表来获取索引。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.array([i for i in range(len(weather))])\n",
    "TAVG_idx = # 在此处输入代码 \n",
    "TMAX_idx = # 在此处输入代码 \n",
    "TMIN_idx = # 在此处输入代码 \n",
    "TAVG_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 示例输出\n",
    "\n",
    "```\n",
    "array([ 3956, 3957, 3958, 3959, 3960, 3961, 3962, 3963, 3964,\n",
    "        3965, 3966, 3967, 3968, 3969, 3970, 3971, 3972, 3973,\n",
    "        3974, 3975, 3976, 3977, 3978, 3979, 3980, 3981, 3982,\n",
    "        3983, 3984, 3985, 4017, 4018, 4019, 4020, 4021, 4022,\n",
    "        4023, 4024, 4025, 4026, 4027, 4028, 4029, 4030, 4031,\n",
    "        4032, 4033, 4034, 4035, 4036, 4037, 4038, 4039, 4040,\n",
    "        4041, 4042, 4043, 4044, 4045, 4046, 4047, 13420])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "您可以用特定气象站/机场的平均值来替换缺少的 TAVG、TMAX 和 TMIN 值。由于 TAVG_idx 缺少连续的行，因此无法用上个值进行替换。请使用均值进行替换。使用 `groupby` 函数来聚合采用均值的变量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_impute = weather.groupby([<CODE>]).agg({'TAVG':'mean','TMAX':'mean', 'TMIN':'mean' }).reset_index()# 在此处输入代码\n",
    "weather_impute.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "合并均值数据与天气数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 获得昨天的数据\n",
    "weather = pd.merge(weather, weather_impute, how='left', left_on=['MONTH','STATION'], right_on = ['MONTH','STATION'])\\\n",
    ".rename(columns = {'TAVG_y':'TAVG_AVG',\n",
    "                   'TMAX_y':'TMAX_AVG',\n",
    "                   'TMIN_y':'TMIN_AVG',\n",
    "                   'TAVG_x':'TAVG',\n",
    "                   'TMAX_x':'TMAX',\n",
    "                   'TMIN_x':'TMIN'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再次检查缺少的值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.TAVG[TAVG_idx] = weather.TAVG_AVG[TAVG_idx]\n",
    "weather.TMAX[TMAX_idx] = weather.TMAX_AVG[TMAX_idx]\n",
    "weather.TMIN[TMIN_idx] = weather.TMIN_AVG[TMIN_idx]\n",
    "weather.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从数据集里删除 `STATION,MONTH,TAVG_AVG,TMAX_AVG,TMIN_AVG,TMAX,TMIN,SNWD`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.drop(columns=['STATION','MONTH','TAVG_AVG', 'TMAX_AVG', 'TMIN_AVG', 'TMAX' ,'TMIN', 'SNWD'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将出发地和目的地天气状况添加到数据集内。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 添加起点站天气状况\n",
    "data_orig = pd.merge(data_orig, weather, how='left', left_on=['FlightDate','Origin'], right_on = ['DATE','airport'])\\\n",
    ".rename(columns = {'AWND':'AWND_O','PRCP':'PRCP_O', 'TAVG':'TAVG_O', 'SNOW': 'SNOW_O'})\\\n",
    ".drop(columns=['DATE','airport'])\n",
    "\n",
    "### 添加目的地天气状况\n",
    "data_orig = pd.merge(data_orig, weather, how='left', left_on=['FlightDate','Dest'], right_on = ['DATE','airport'])\\\n",
    ".rename(columns = {'AWND':'AWND_D','PRCP':'PRCP_D', 'TAVG':'TAVG_D', 'SNOW': 'SNOW_D'})\\\n",
    ".drop(columns=['DATE','airport'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意**：在合并数据后最好先检查空值/缺失值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(data.isna().any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_orig.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用独热编码将分类数据转化为数值数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_orig.copy()\n",
    "data = data[['Year', 'Quarter', 'Month', 'DayofMonth', 'DayOfWeek',\n",
    "       'Reporting_Airline', 'Origin', 'Dest','Distance','DepHourofDay', 'is_delay', 'is_holiday', 'AWND_O', 'PRCP_O',\n",
    "       'TAVG_O', 'AWND_D', 'PRCP_D', 'TAVG_D', 'SNOW_O', 'SNOW_D']]\n",
    "\n",
    "\n",
    "categorical_columns = ['Year', 'Quarter', 'Month', 'DayofMonth', 'DayOfWeek',\n",
    "       'Reporting_Airline', 'Origin', 'Dest', 'is_holiday','is_delay']\n",
    "for c in categorical_columns:\n",
    "    data[c] = data[c].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dummies = # 在此处输入代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 示例代码\n",
    "\n",
    "```\n",
    "data_dummies = pd.get_dummies(data[['Year', 'Quarter', 'Month', 'DayofMonth', 'DayOfWeek', 'Reporting_Airline', 'Origin', 'Dest', 'is_holiday']], drop_first=True)\n",
    "data = pd.concat([data, data_dummies], axis = 1)\n",
    "categorical_columns.remove('is_delay')\n",
    "data.drop(categorical_columns,axis=1, inplace=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "检查新列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 示例输出\n",
    "\n",
    "```\n",
    "Index(['Distance', 'DepHourofDay', 'is_delay', 'AWND_O', 'PRCP_O', 'TAVG_O',\n",
    "       'AWND_D', 'PRCP_D', 'TAVG_D', 'SNOW_O', 'SNOW_D', 'Year_2015',\n",
    "       'Year_2016', 'Year_2017', 'Year_2018', 'Quarter_2', 'Quarter_3',\n",
    "       'Quarter_4', 'Month_2', 'Month_3', 'Month_4', 'Month_5', 'Month_6',\n",
    "       'Month_7', 'Month_8', 'Month_9', 'Month_10', 'Month_11', 'Month_12',\n",
    "       'DayofMonth_2', 'DayofMonth_3', 'DayofMonth_4', 'DayofMonth_5',\n",
    "       'DayofMonth_6', 'DayofMonth_7', 'DayofMonth_8', 'DayofMonth_9',\n",
    "       'DayofMonth_10', 'DayofMonth_11', 'DayofMonth_12', 'DayofMonth_13',\n",
    "       'DayofMonth_14', 'DayofMonth_15', 'DayofMonth_16', 'DayofMonth_17',\n",
    "       'DayofMonth_18', 'DayofMonth_19', 'DayofMonth_20', 'DayofMonth_21',\n",
    "       'DayofMonth_22', 'DayofMonth_23', 'DayofMonth_24', 'DayofMonth_25',\n",
    "       'DayofMonth_26', 'DayofMonth_27', 'DayofMonth_28', 'DayofMonth_29',\n",
    "       'DayofMonth_30', 'DayofMonth_31', 'DayOfWeek_2', 'DayOfWeek_3',\n",
    "       'DayOfWeek_4', 'DayOfWeek_5', 'DayOfWeek_6', 'DayOfWeek_7',\n",
    "       'Reporting_Airline_DL', 'Reporting_Airline_OO', 'Reporting_Airline_UA',\n",
    "       'Reporting_Airline_WN', 'Origin_CLT', 'Origin_DEN', 'Origin_DFW',\n",
    "       'Origin_IAH', 'Origin_LAX', 'Origin_ORD', 'Origin_PHX', 'Origin_SFO',\n",
    "       'Dest_CLT', 'Dest_DEN', 'Dest_DFW', 'Dest_IAH', 'Dest_LAX', 'Dest_ORD',\n",
    "       'Dest_PHX', 'Dest_SFO', 'is_holiday_1'],\n",
    "      dtype='object')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再次将 `is_delay` 列重命名为 `target`。使用跟之前一样的代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rename(columns = {<CODE>:<CODE>}, inplace=True )# 在此处输入代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再次创建训练集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在此处输入代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 新基准分类器\n",
    "\n",
    "现在我们来看一下这些新特征是否提高了模型的预测能力。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实例化线性学习器评估程序对象\n",
    "num_classes = # 在此处输入代码\n",
    "classifier_estimator = # 在此处输入代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 示例代码\n",
    "\n",
    "```\n",
    "num_classes = len(pd.unique(train_labels)) \n",
    "classifier_estimator = sagemaker.LinearLearner(role=sagemaker.get_execution_role(),\n",
    "                                               train_instance_count=1,\n",
    "                                               train_instance_type='ml.m4.xlarge',\n",
    "                                               predictor_type='binary_classifier',\n",
    "                                              binary_classifier_model_selection_criteria = 'cross_entropy_loss')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_records = classifier_estimator.record_set(train_features, train_labels, channel='train')\n",
    "val_records = classifier_estimator.record_set(val_features, val_labels, channel='validation')\n",
    "test_records = classifier_estimator.record_set(test_features, test_labels, channel='test')\n",
    "\n",
    "classifier_estimator.fit([train_records, val_records, test_records])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_predictor = # 在此处输入代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_batches(classifier_predictor, test_features, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "线性模型的效果只有少许提高。我们使用 Amazon SageMaker 来尝试基于树的集成模型 XGBoost。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 尝试 XGBoost 模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "您需要采取的步骤如下：  \n",
    "\n",
    "1. 使用训练集变量并将其另存为 CSV 文件 train.csv、validation.csv 和 test.csv  \n",
    "2. 将存储桶名称存储在变量中。Amazon S3 存储桶名称位于实验说明左侧。 \n",
    "a. `bucket = <LabBucketName>`  \n",
    "b. `prefix = 'sagemaker/xgboost'`  \n",
    "3. 使用 Boto3 将模型上传到存储桶。   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, validation_data, test_data = np.split(data.sample(frac=1, random_state=1729), [int(0.7 * len(data)), int(0.9*len(data))])  \n",
    "\n",
    "pd.concat([train_data['target'], train_data.drop(['target'], axis=1)], axis=1).to_csv('train.csv', index=False, header=False)\n",
    "pd.concat([validation_data['target'], validation_data.drop(['target'], axis=1)], axis=1).to_csv('validation.csv', index=False, header=False)\n",
    "pd.concat([test_data['target'], test_data.drop(['target'], axis=1)], axis=1).to_csv('test.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用实验账户提供的资源名称替换 **`<LabBucketName>`**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在此处输入代码，以定义存储桶和 prefix\n",
    "bucket = <LabBucketName> # 在此处输入代码\n",
    "prefix = 'sagemaker/xgboost'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 将数据上传到 S3\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train/train.csv')).upload_file('train.csv')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'validation/validation.csv')).upload_file('validation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "container = get_image_uri(boto3.Session().region_name, 'xgboost')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用 `sagemaker.s3_input` 函数为训练和验证数据集创建一个 record_set。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_input_train = sagemaker.s3_input(s3_data='s3://{}/{}/train'.format(bucket, prefix), content_type='csv')\n",
    "s3_input_validation = sagemaker.s3_input(s3_data='s3://{}/{}/validation/'.format(bucket, prefix), content_type='csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "\n",
    "xgb = sagemaker.estimator.Estimator(container,\n",
    "                                    role = sagemaker.get_execution_role(),\n",
    "                                    train_instance_count=1,\n",
    "                                    train_instance_type='ml.m4.xlarge',\n",
    "                                    output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
    "                                    sagemaker_session=sess)\n",
    "xgb.set_hyperparameters(max_depth=5,\n",
    "                        eta=0.2,\n",
    "                        gamma=4,\n",
    "                        min_child_weight=6,\n",
    "                        subsample=0.8,\n",
    "                        silent=0,\n",
    "                        objective='binary:logistic',\n",
    "                        eval_metric = \"auc\",\n",
    "                        num_round=100)\n",
    "\n",
    "xgb.fit({'train': s3_input_train, 'validation': s3_input_validation})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "针对新模型部署预测器并使用测试数据集来评估模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor = xgb.deploy(initial_instance_count = <CODE>,\n",
    "                           instance_type = <CODE>)# 在此处输入代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.concat([test_data['target'], test_data.drop(['target'], axis=1)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor.content_type = 'text/csv'\n",
    "xgb_predictor.serializer = csv_serializer\n",
    "xgb_predictor.deserializer = None\n",
    "\n",
    "\n",
    "def predict(predictor , features, labels , prob_threshold = 0.5, rows=500):\n",
    "    \"\"\"\n",
    "    返回评估结果\n",
    "    predictor：模型的预测器对象\n",
    "    features：输入模型的特征\n",
    "    label：Ground Truth 目标值\n",
    "    prob_threshold：分隔阳性和阴性类的概率分界点\n",
    "    \"\"\"\n",
    "    split_array = np.array_split(features, int(features.shape[0] / float(rows) + 1))\n",
    "    predictions = ''\n",
    "    for array in split_array:\n",
    "        predictions = ','.join([predictions, predictor.predict(array).decode('utf-8')])\n",
    "\n",
    "    preds = np.fromstring(predictions[1:], sep=',')\n",
    "    preds = preds.reshape((-1,))\n",
    "    predictions = np.where(preds > prob_threshold , 1, 0)\n",
    "    labels = labels.reshape((-1,))\n",
    "    \n",
    "    \n",
    "    # 计算准确率\n",
    "    accuracy = (predictions == labels).sum() / labels.shape[0]\n",
    "    print(f'Accuracy: {accuracy}')\n",
    "    \n",
    "    auc = roc_auc_score(labels, preds)\n",
    "    print(f'AUC : {auc}')\n",
    "    \n",
    "    precision, recall, f1_score, _ = precision_recall_fscore_support(labels, predictions, average = 'binary')\n",
    "    print(f'Precision: {precision}')\n",
    "    print(f'Recall: {recall}')\n",
    "    print(f'F1_score: {f1_score}')\n",
    "    \n",
    "    \n",
    "    confusion_matrix = pd.crosstab(index=labels, columns=np.round(preds), rownames=['True'], colnames=['predictions']).astype(int)\n",
    "    plt.figure(figsize = (5,5))\n",
    "    sns.heatmap(confusion_matrix, annot=True, fmt='.2f', cmap=\"YlGnBu\").set_title('Confusion Matrix') \n",
    "    return list(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict(xgb_predictor, test_data.as_matrix()[:, 1:] , test_data.iloc[:, 0:1].as_matrix(), prob_threshold = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 尝试不同阈值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict(xgb_predictor, test_data.as_matrix()[:, 1:] , test_data.iloc[:, 0:1].as_matrix(), prob_threshold = 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**问题**：您能从模型使用测试集的效果中推断出什么？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在此处输入答案"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 超参数优化 (HPO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner\n",
    "\n",
    "### 您可以启动多个实例同时进行超参数优化\n",
    "\n",
    "xgb = sagemaker.estimator.Estimator(container,\n",
    "                                    role=sagemaker.get_execution_role(),\n",
    "                                    train_instance_count= 2, # 确保针对这些实例设置了限制\n",
    "                                    train_instance_type='ml.m4.xlarge',\n",
    "                                    output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
    "                                    sagemaker_session=sess)\n",
    "\n",
    "xgb.set_hyperparameters(eval_metric='auc',\n",
    "                        objective='binary:logistic',\n",
    "                        num_round=100,\n",
    "                        rate_drop=0.3,\n",
    "                        tweedie_variance_power=1.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_ranges = {'eta': ContinuousParameter(0, 0.5),\n",
    "                        'min_child_weight': ContinuousParameter(3, 10),\n",
    "                         'num_round': IntegerParameter(10, 150),\n",
    "                        'alpha': ContinuousParameter(0, 2),\n",
    "                        'max_depth': IntegerParameter(10, 15)}\n",
    "\n",
    "objective_metric_name = 'validation:auc'\n",
    "\n",
    "tuner = HyperparameterTuner(xgb,\n",
    "                            objective_metric_name,\n",
    "                            hyperparameter_ranges,\n",
    "                            max_jobs=10,\n",
    "                            max_parallel_jobs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.fit({'train': s3_input_train, 'validation': s3_input_validation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boto3.client('sagemaker').describe_hyper_parameter_tuning_job(\n",
    "    HyperParameterTuningJobName=tuner.latest_tuning_job.job_name)['HyperParameterTuningJobStatus']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i class=\"fas fa-exclamation-triangle\" style=\"color:red\"></i> 等待训练作业完成。这可能需要 25 到 30 分钟的时间。\n",
    "\n",
    "**要监控超参数优化作业，您需要进行以下操作：**  \n",
    "\n",
    "1. 在 AWS 管理控制台的 **Services（服务）**菜单中，单击 **Amazon SageMaker**。 \n",
    "1. 单击 **Training（训练）**> **Hyperparameter tuning jobs（超参数优化作业）**。 \n",
    "1. 您可以查看每个超参数优化作业的状态、目标指标值和日志。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sage_client = boto3.Session().client('sagemaker')\n",
    "tuning_job_name = tuner.latest_tuning_job.job_name\n",
    "\n",
    "#运行以下单元，查看超参数优化作业的当前状态。\n",
    "tuning_job_result = sage_client.describe_hyper_parameter_tuning_job(HyperParameterTuningJobName=tuning_job_name)\n",
    "\n",
    "status = tuning_job_result['HyperParameterTuningJobStatus']\n",
    "if status != 'Completed':\n",
    "    print('Reminder: the tuning job has not been completed.')\n",
    "    \n",
    "job_count = tuning_job_result['TrainingJobStatusCounters']['Completed']\n",
    "print(\"%d training jobs have completed\" % job_count)\n",
    "    \n",
    "is_minimize = (tuning_job_result['HyperParameterTuningJobConfig']['HyperParameterTuningJobObjective']['Type'] != 'Maximize')\n",
    "objective_name = tuning_job_result['HyperParameterTuningJobConfig']['HyperParameterTuningJobObjective']['MetricName']\n",
    "\n",
    "\n",
    "from pprint import pprint\n",
    "if tuning_job_result.get('BestTrainingJob',None):\n",
    "    print(\"Best model found so far:\")\n",
    "    pprint(tuning_job_result['BestTrainingJob'])\n",
    "else:\n",
    "    print(\"No training jobs have reported results yet.\")\n",
    "    \n",
    "### 获取 DataFrame 中的超参数优化作业结果\n",
    "tuner_df = sagemaker.HyperparameterTuningJobAnalytics(tuning_job_name).dataframe()\n",
    "\n",
    "if len(tuner_df) > 0:\n",
    "    df = tuner_df[tuner_df['FinalObjectiveValue'] > -float('inf')]\n",
    "    if len(df) > 0:\n",
    "        df = df.sort_values('FinalObjectiveValue', ascending=is_minimize)\n",
    "        print(\"Number of training jobs with valid objective: %d\" % len(df))\n",
    "        print({\"lowest\":min(df['FinalObjectiveValue']),\"highest\": max(df['FinalObjectiveValue'])})\n",
    "        pd.set_option('display.max_colwidth', -1) # 请勿截断 TrainingJobName        \n",
    "    else:\n",
    "        print(\"No training jobs have reported valid results yet.\")\n",
    "        \n",
    "tuner_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "部署经过超参数优化训练的最佳模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor_hpo = # 在此处输入代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor_hpo.content_type = 'text/csv'\n",
    "xgb_predictor_hpo.serializer = csv_serializer\n",
    "xgb_predictor_hpo.deserializer = None\n",
    "\n",
    "predictions = predict(xgb_predictor_hpo, test_data.as_matrix()[:, 1:] , test_data.iloc[:, 0:1].as_matrix(), prob_threshold = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**问题**：尝试不同的超参数和超参数范围。这会改进模型吗？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特征重要性\n",
    "\n",
    "所有超参数优化作业的模型文件都按上表中训练作业名称的顺序保存在“{bucket}/{prefix}/output/”文件夹中。您可以加载这些模型，并像使用常规 sklearn 模型对象一样对其进行解释。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_hpo_model_path = \"s3://\" + bucket + \"/sagemaker/xgboost/output/<Best model TrainingJobName>/output/model.tar.gz\"\n",
    "best_hpo_model_path = <path-to-your-model>\n",
    "### 将 best_hpo_model_path 下载到本地\n",
    "!aws s3 cp {best_hpo_model_path} ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安装 xgboost\n",
    "!pip install xgboost=='0.90'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 加载选择的模型文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import tarfile\n",
    "import xgboost\n",
    "\n",
    "with open('model.tar.gz', 'rb') as f:\n",
    "    with tarfile.open(fileobj=f, mode='r') as tar_f:\n",
    "        with tar_f.extractfile('xgboost-model') as extracted_f:\n",
    "            xgbooster = pickle.load(extracted_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 将列名映射到 XGBoost 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = list(data.columns)\n",
    "columns.remove('target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = xgbooster.get_fscore()\n",
    "feature_importance_col = {}\n",
    "\n",
    "for column, fname in zip(columns, xgbooster.feature_names):\n",
    "    try:\n",
    "         feature_importance_col[column] = feature_importance[fname]\n",
    "    except Exception:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 按特征重要性值进行排序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(feature_importance_col.items(), key=lambda kv: kv[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 示例输出\n",
    "\n",
    "```\n",
    "[('AWND_O', 13851),\n",
    " ('AWND_D', 13452),\n",
    " ('DepHourofDay', 13344),\n",
    " ('TAVG_O', 13106),\n",
    " ('TAVG_D', 12800),\n",
    " ('Distance', 8478),\n",
    " ('PRCP_O', 4210),\n",
    " ('PRCP_D', 3916),\n",
    " ('Reporting_Airline_UA', 1791),\n",
    " ('Year_2016', 1290),\n",
    " ('Year_2015', 1285),\n",
    " ('Year_2018', 1157),\n",
    " ('Quarter_4', 1092),\n",
    " ('Year_2017', 1009),\n",
    " ('DayOfWeek_5', 838),\n",
    " ('DayOfWeek_4', 833),\n",
    " ('Quarter_2', 799),\n",
    " ('DayOfWeek_7', 783),\n",
    " ('Reporting_Airline_WN', 736),\n",
    " ('DayOfWeek_3', 718),\n",
    " ('Origin_ORD', 706),\n",
    " ('DayOfWeek_2', 685),\n",
    " ('Reporting_Airline_DL', 663),\n",
    " ('Dest_LAX', 627),\n",
    " ('DayOfWeek_6', 614),\n",
    " ('Reporting_Airline_OO', 596),\n",
    " ('Origin_LAX', 590),\n",
    " ('Month_11', 565),\n",
    " ('Month_5', 543),\n",
    " ('Dest_ORD', 539),\n",
    " ('SNOW_O', 506),\n",
    " ('Month_8', 497),\n",
    " ('Month_12', 495),\n",
    " ('Month_7', 484),\n",
    " ('Origin_DFW', 480),\n",
    " ('Quarter_3', 477),\n",
    " ('Dest_DFW', 462),\n",
    " ('Origin_DEN', 456),\n",
    " ('Origin_SFO', 438),\n",
    " ('Month_6', 437),\n",
    " ('Dest_SFO', 437),\n",
    " ('Month_10', 422),\n",
    " ('Month_9', 407),\n",
    " ('Month_4', 394),\n",
    " ('Dest_DEN', 379),\n",
    " ('SNOW_D', 375),\n",
    " ('Month_3', 369),\n",
    " ('Dest_IAH', 369),\n",
    " ('Origin_PHX', 347),\n",
    " ('Dest_PHX', 346),\n",
    " ('Origin_IAH', 342),\n",
    " ('DayofMonth_15', 324),\n",
    " ('DayofMonth_17', 322),\n",
    " ('DayofMonth_21', 322),\n",
    " ('Origin_CLT', 322),\n",
    " ('DayofMonth_20', 316),\n",
    " ('DayofMonth_9', 313),\n",
    " ('DayofMonth_19', 307),\n",
    " ('DayofMonth_26', 303),\n",
    " ('DayofMonth_7', 301),\n",
    " ('DayofMonth_4', 300),\n",
    " ('DayofMonth_2', 299),\n",
    " ('DayofMonth_27', 298),\n",
    " ('DayofMonth_3', 294),\n",
    " ('DayofMonth_28', 287),\n",
    " ('Month_2', 284),\n",
    " ('DayofMonth_5', 284),\n",
    " ('DayofMonth_12', 282),\n",
    " ('DayofMonth_11', 276),\n",
    " ('Dest_CLT', 276),\n",
    " ('DayofMonth_6', 275),\n",
    " ('DayofMonth_22', 274),\n",
    " ('DayofMonth_8', 273),\n",
    " ('DayofMonth_18', 273),\n",
    " ('DayofMonth_29', 267),\n",
    " ('DayofMonth_23', 266),\n",
    " ('DayofMonth_14', 263),\n",
    " ('DayofMonth_30', 260),\n",
    " ('DayofMonth_10', 259),\n",
    " ('DayofMonth_16', 257),\n",
    " ('DayofMonth_24', 255),\n",
    " ('DayofMonth_13', 245),\n",
    " ('DayofMonth_25', 237),\n",
    " ('is_holiday_1', 231),\n",
    " ('DayofMonth_31', 164)]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据上述特征重要性，您可以发现，出发地和目的地的 DepHourofDay、Airwind 和 Temperature 是决定航班是否延误的主要影响因素。\n",
    "\n",
    "通过这种方式，我们验证了在特征方面有哪些直观的信息，以及模型实际实际学习到了什么。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "现在，您已经至少完成了几次训练和评估模型的迭代。我们来总结一下该项目，回顾自己学到的内容，并思考可以采取哪些步骤来继续学习（假设您还有时间）。使用下面的单元格回答其中一些问题和其他相关问题：\n",
    "\n",
    "1. 您的模型性能是否能实现您的业务目标？ 如果不符合，而且您还有时间进行优化，那么您会采取哪些不同的做法？\n",
    "2. 在您对数据集、特征和超参数做出更改之后，模型有多大程度的改进？ 在整个项目中，您认为自己采用的哪些方法对模型的改进最大？\n",
    "3. 在整个项目中，您遇到的最大挑战是什么？\n",
    "4. 管道的某些方面对您来说没有意义，关于这些方面，您有没有未解决的问题？\n",
    "5.在完成这个项目的过程中，关于机器学习，您学到的三件最重要的事情是什么？\n",
    "\n",
    "#### <span style=\"color: blue;\">项目演示：同样在项目演示中总结这些问题的答案。现在，请整理您为项目演示做下的所有笔记，并准备向全班展示您的成果。</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
