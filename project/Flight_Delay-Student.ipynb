{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem: Predicting Airplane Delays\n",
    "\n",
    "The multi-fold goals of this notebook are:\n",
    "- Process and create a dataset from downloaded ZIP files\n",
    "- Exploratory data analysis (EDA)\n",
    "- Establish a baseline model\n",
    "- Move from a simple model to an ensemble model\n",
    "- Hyperparameter optimization\n",
    "- Check feature importance\n",
    "\n",
    "## Introduction to business scenario\n",
    "You work for a travel booking website that is working to improve the customer experience for flights that were delayed. The company wants to create a feature to let customers know if the flight will be delayed due to weather when the customers are booking the flight to or from the busiest airports for domestic travel in the US. \n",
    "\n",
    "You are tasked with solving part of this problem by leveraging machine learning to identify whether the flight will be delayed due to weather. You have been given access to the a dataset of on-time performance of domestic flights operated by large air carriers. You can use this data to train a machine learning model to predict if the flight is going to be delayed for the busiest airports.\n",
    "\n",
    "## About this dataset\n",
    "This dataset contains scheduled and actual departure and arrival times reported by certified US air carriers that account for at least 1% of domestic scheduled passenger revenues. The data was collected by the Office of Airline Information, Bureau of Transportation Statistics (BTS). The dataset contains date, time, origin, destination, airline, distance, and delay status of flights for flights between 2013 and 2018.\n",
    "\n",
    "### Features\n",
    "For more information about features in the dataset, see [On-time delay dataset features](https://www.transtats.bts.gov/Fields.asp).\n",
    "\n",
    "### Dataset attributions  \n",
    "Website: https://www.transtats.bts.gov/\n",
    "\n",
    "Dataset(s) used in this lab were compiled by the Office of Airline Information, Bureau of Transportation Statistics (BTS), Airline On-Time Performance Data, available at https://www.transtats.bts.gov/DatabaseInfo.asp?DB_ID=120&DB_URL=Mode_ID=1&Mode_Desc=Aviation&Subject_ID2=0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Problem formulation and data collection\n",
    "\n",
    "Start this project off by writing a few sentences below that summarize the business problem and the business goal you're trying to achieve in this scenario. Include a business metric you would like your team to aspire toward. With that information defined, clearly write out the machine learning problem statement. Finally, add a comment or two about the type of machine learning this represents. \n",
    "\n",
    "#### <span style=\"color: blue;\">Project presentation: Include a summary of these details in your project presentations.</span>\n",
    "\n",
    "### 1. Determine if and why ML is an appropriate solution to deploy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Formulate the business problem, success metrics, and desired ML output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Identify the type of ML problem you’re dealing with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Analyze the appropriateness of the data you’re working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "Now that we have decided where to focus our energy, let's set things up so you can start working on solving the problem.\n",
    "\n",
    "**Note:** This notebook was created and tested on an `ml.m4.xlarge` notebook instance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib2 import Path\n",
    "from zipfile import ZipFile\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import subprocess\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Data preprocessing and visualization  \n",
    "In this data preprocessing phase, you should take the opportunity to explore and visualize your data to better understand it. First, import the necessary libraries and read the data into a Pandas dataframe. After that, explore your data. Look for the shape of the dataset and explore your columns and the types of columns you're working with (numerical, categorical). Consider performing basic statistics on the features to get a sense of feature means and ranges. Take a close look at your target column and determine its distribution.\n",
    "\n",
    "### Specific questions to consider\n",
    "1. What can you deduce from the basic statistics you ran on the features? \n",
    "\n",
    "2. What can you deduce from the distributions of the target classes?\n",
    "\n",
    "3. Is there anything else you deduced from exploring the data?\n",
    "\n",
    "#### <span style=\"color: blue;\">Project presentation: Include a summary of your answers to these and other similar questions in your project presentations.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by bringing in the dataset from an Amazon S3 public bucket to this notebook environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether the file is already in the desired path or if it needs to be downloaded\n",
    "\n",
    "base_path = '/home/ec2-user/SageMaker/project/data/FlightDelays/'\n",
    "csv_base_path = '/home/ec2-user/SageMaker/project/data/csvFlightDelays/'\n",
    "file_path = 'On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2014_1.zip'\n",
    "\n",
    "if not os.path.isfile(base_path + file_path):\n",
    "    subprocess.run(['mkdir', '-p', base_path])\n",
    "    subprocess.run(['mkdir', '-p', csv_base_path])\n",
    "    subprocess.run(['aws', 's3', 'cp', \n",
    "                    's3://aws-tc-largeobjects/ILT-TF-200-MLDWTS/flight_delay_project/csvFlightData-5/', \n",
    "                    base_path,'--recursive'])\n",
    "else:\n",
    "    print('File already downloaded!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_files = [str(file) for file in list(Path(base_path).iterdir()) if '.zip' in str(file)]\n",
    "len(zip_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract CSV files from ZIP files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def zip2csv(zipFile_name , file_path = '/home/ec2-user/SageMaker/project/data/csvFlightDelays'):\n",
    "    \"\"\"\n",
    "    Extract csv from zip files\n",
    "    zipFile_name: name of the zip file\n",
    "    file_path : name of the folder to store csv\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with ZipFile(zipFile_name, 'r') as z: \n",
    "            print(f'Extracting {zipFile_name} ') \n",
    "            z.extractall(path=file_path) \n",
    "    except:\n",
    "        print(f'zip2csv failed for {zipFile_name}')\n",
    "\n",
    "for file in zip_files:\n",
    "    zip2csv(file)\n",
    "\n",
    "print(\"Files Extracted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_files = [str(file) for file in list(Path(csv_base_path).iterdir()) if '.csv' in str(file)]\n",
    "len(csv_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before loading the CSV file, read the HTML file from the extracted folder. This HTML file includes the background and more information on the features included in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame(src=\"./data/csvFlightDelays/readme.html\", width=1000, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load sample CSV\n",
    "\n",
    "Before combining all the CSV files, get a sense of the data from a single CSV file. Using Pandas, read the `On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2018_9.csv` file first. You can use the Python built-in `read_csv` function ([documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = pd.read_csv(<CODE> # **ENTER YOUR CODE HERE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Print the row and column length in the dataset, and print the column names.\n",
    "\n",
    "**Hint**: Use the `<dataframe>.shape` function to view the rows and columns of a dataframe and `<dataframe>.columns` to view the column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shape = # **ENTER YOUR CODE HERE**\n",
    "print(f'Rows and columns in one csv file is {df_shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Print the first 10 rows of the dataset.  \n",
    "\n",
    "**Hint**: Use the built-in Pandas function `head(x)` to print `x` number of rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Print all the columns in the dataset. Use `<dataframe>.columns` to view the column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The column names are :')\n",
    "print('#########')\n",
    "for col in <CODE>:# **ENTER YOUR CODE HERE**\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Print all the columns in the dataset that contain the word 'Del'. This will help you see how many columns have delay data in them.\n",
    "\n",
    "**Hint**: You can use a Python list comprehension to include values that pass certain `if` statement criteria.\n",
    "\n",
    "For example: `[x for x in [1,2,3,4,5] if x > 2]`  \n",
    "\n",
    "**Hint**: You can use the `in` keyword ([documentation](https://www.w3schools.com/python/ref_keyword_in.asp)) to check if the value is in a list or not. \n",
    "\n",
    "For example: `5 in [1,2,3,4,5]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some more questions to help you find out more about your dataset.\n",
    "\n",
    "**Questions**   \n",
    "1. How many rows and columns does the dataset have?   \n",
    "2. How many years are included in the dataset?   \n",
    "3. What is the date range for the dataset?   \n",
    "4. Which airlines are included in the dataset?   \n",
    "5. Which origin and destination airports are covered?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The #rows and #columns are \", <CODE> , \" and \", <CODE>)\n",
    "print(\"The years in this dataset are: \", <CODE>)\n",
    "print(\"The months covered in this dataset are: \", <CODE>)\n",
    "print(\"The date range for data is :\" , min(<CODE>), \" to \", max(<CODE>))\n",
    "print(\"The airlines covered in this dataset are: \", list(<CODE>))\n",
    "print(\"The Origin airports covered are: \", list(<CODE>))\n",
    "print(\"The Destination airports covered are: \", list(<CODE>))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: What is the count of all the origin and destination airports?\n",
    "\n",
    "**Hint**: You can use the Pandas `values_count` function ([documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.value_counts.html)) to find out the values for each airport using the columns `Origin` and `Dest`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = pd.DataFrame({'Origin':<CODE>, 'Destination':<CODE>})\n",
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Print the top 15 origin and destination airports based on number of flights in the dataset.\n",
    "\n",
    "**Hint**: You can use the Pandas `sort_values` function ([documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sort_values.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts.sort_values(by=<CODE>,ascending=False).head(15 )# Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Given all the information about a flight trip, can you predict if it would be delayed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, assume you are traveling from San Francisco to Los Angeles on a work trip. You want to have an ideas if your flight will be delayed, given a set of features, so that you can manage your reservations in Los Angeles better. How many features from this dataset would you know before your flight?\n",
    "\n",
    "Columns such as `DepDelay`, `ArrDelay`, `CarrierDelay`, `WeatherDelay`, `NASDelay`, `SecurityDelay`, `LateAircraftDelay`, and `DivArrDelay` contain information about a delay. But this delay could have occured at the origin or destination. If there were a sudden weather delay 10 minutes before landing, this data would not be helpful in managing your Los Angeles reservations.\n",
    "\n",
    "So to simplify the problem statement, consider the following columns to predict an arrival delay:<br>\n",
    "\n",
    "`Year`, `Quarter`, `Month`, `DayofMonth`, `DayOfWeek`, `FlightDate`, `Reporting_Airline`, `Origin`, `OriginState`, `Dest`, `DestState`, `CRSDepTime`, `DepDelayMinutes`, `DepartureDelayGroups`, `Cancelled`, `Diverted`, `Distance`, `DistanceGroup`, `ArrDelay`, `ArrDelayMinutes`, `ArrDel15`, `AirTime`\n",
    "\n",
    "You will also filter the source and destination airports to be:\n",
    "- Top airports: ATL, ORD, DFW, DEN, CLT, LAX, IAH, PHX, SFO\n",
    "- Top 5 airlines: UA, OO, WN, AA, DL\n",
    "\n",
    "This should help in reducing the size of data across the CSV files to be combined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine all CSV files\n",
    "\n",
    "**Hint**:  \n",
    "First, create an empy dataframe that you will use to copy your individual dataframes from each file. Then, for each file in the `csv_files` list:\n",
    "\n",
    "1. Read the CSV file into a dataframe  \n",
    "2. Filter the columns based on the `filter_cols` variable\n",
    "\n",
    "```\n",
    "        columns = ['col1', 'col2']\n",
    "        df_filter = df[columns]\n",
    "```\n",
    "\n",
    "3. Keep only the subset_vals in each of the subset_cols. Use the `isin` Pandas function ([documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.isin.html)) to check if the `val` is in the dataframe column and then choose the rows that include it.\n",
    "\n",
    "```\n",
    "        df_eg[df_eg['col1'].isin('5')]\n",
    "```\n",
    "\n",
    "4. Concatenate the dataframe with the empty dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_csv(csv_files, filter_cols, subset_cols, subset_vals, file_name = 'data/combined_files.csv'):\n",
    "    \"\"\"\n",
    "    Combine csv files into one Data Frame\n",
    "    csv_files: list of csv file paths\n",
    "    filter_cols: list of columns to filter\n",
    "    subset_cols: list of columns to subset rows\n",
    "    subset_vals: list of list of values to subset rows\n",
    "    \"\"\"\n",
    "    # Create an empty dataframe\n",
    "    df = # Enter your code here \n",
    "    \n",
    "    for file in csv_files:\n",
    "        # Read the CSV file into a dataframe\n",
    "        df_temp = pd.read_csv(<CODE>)# Enter your code here\n",
    "        \n",
    "        # Filter the columns based on the filter_cols variable\n",
    "        # e.g. columns = ['col1', 'col2']\n",
    "        # df_filter = df[columns]\n",
    "        df_temp = # Enter your code here\n",
    "        \n",
    "        # Keep only the subset_vals in each of the subset_cols\n",
    "        # HINT: Use the `isin` function to check if the val is in dataframe column\n",
    "        # and then choose the rows that include it\n",
    "        # e.g. df[df['col1'].isin('5')]\n",
    "        for col, val in zip(subset_cols,subset_vals):\n",
    "            df_temp = # Enter your code here     \n",
    "        \n",
    "        # Use Pandas concatenate `pd.concat` to concatenate the main dataframe with the dataframe for each file\n",
    "        df = pd.concat([df, df_temp], axis=0)\n",
    "    \n",
    "        \n",
    "    df.to_csv(file_name, index=False)\n",
    "    print(f'Combined csv stored at {file_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cols is the list of columns to predict Arrival Delay \n",
    "cols = ['Year','Quarter','Month','DayofMonth','DayOfWeek','FlightDate',\n",
    "        'Reporting_Airline','Origin','OriginState','Dest','DestState',\n",
    "        'CRSDepTime','Cancelled','Diverted','Distance','DistanceGroup',\n",
    "        'ArrDelay','ArrDelayMinutes','ArrDel15','AirTime']\n",
    "\n",
    "subset_cols = ['Origin', 'Dest', 'Reporting_Airline']\n",
    "\n",
    "# subset_vals is a list collection of the top origin and destination airports and top 5 airlines\n",
    "subset_vals = [['ATL', 'ORD', 'DFW', 'DEN', 'CLT', 'LAX', 'IAH', 'PHX', 'SFO'], \n",
    "               ['ATL', 'ORD', 'DFW', 'DEN', 'CLT', 'LAX', 'IAH', 'PHX', 'SFO'], \n",
    "               ['UA', 'OO', 'WN', 'AA', 'DL']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the function above to merge all the different files into a single file that you can read easily. \n",
    "\n",
    "**Note**: This will take 5-7 minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "combine_csv(csv_files, cols, subset_cols, subset_vals)\n",
    "print(f'csv\\'s merged in {round((time.time() - start)/60,2)} minutes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dataset\n",
    "\n",
    "Load the combined dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(<CODE>)# Enter your code here to read the combined csv file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the first 5 records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some more questions to help you find out more about your dataset.\n",
    "\n",
    "**Questions**   \n",
    "1. How many rows and columns does the dataset have?   \n",
    "2. How many years are included in the dataset?   \n",
    "3. What is the date range for the dataset?   \n",
    "4. Which airlines are included in the dataset?   \n",
    "5. Which origin and destination airports are covered?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The #rows and #columns are \", <CODE> , \" and \", <CODE>)\n",
    "print(\"The years in this dataset are: \", list(<CODE>))\n",
    "print(\"The months covered in this dataset are: \", sorted(list(<CODE>)))\n",
    "print(\"The date range for data is :\" , min(<CODE>), \" to \", max(<CODE>))\n",
    "print(\"The airlines covered in this dataset are: \", list(<CODE>))\n",
    "print(\"The Origin airports covered are: \", list(<CODE>))\n",
    "print(\"The Destination airports covered are: \", list(<CODE>))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define our **target column : is_delay** (1 - if arrival time delayed more than 15 minutes, 0 - otherwise). Use the `rename` method to rename the column from `ArrDel15` to `is_delay`.\n",
    "\n",
    "**Hint**: You can use the Pandas `rename` function ([documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.rename.html)).\n",
    "\n",
    "For example:\n",
    "```\n",
    "df.rename(columns={'col1':'column1'}, inplace=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rename(columns=<CODE>, inplace=True) # Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look for nulls across columns. You can use the `isnull()` function ([documentation](https://pandas.pydata.org/pandas-docs/version/0.17.0/generated/pandas.isnull.html)).\n",
    "\n",
    "**Hint**: `isnull()` detects whether the particular value is null or not and gives you a boolean (True or False) in its place. Use the `sum(axis=0)` function to sum up the number of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The arrival delay details and airtime are missing for 22540 out of 1658130 rows, which is 1.3%. You can either remove or impute these rows. The documentation does not mention anything about missing rows.\n",
    "\n",
    "**Hint**: Use the `~` operator to choose the values that aren't null from the `isnull()` output.\n",
    "\n",
    "For example:\n",
    "```\n",
    "null_eg = df_eg[~df_eg['column_name'].isnull()]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Remove null columns\n",
    "data = # Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the hour of the day in 24-hour time format from CRSDepTime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['DepHourofDay'] = # Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **The ML problem statement**\n",
    "- Given a set of features, can you predict if a flight is going to be delayed more than 15 minutes?\n",
    "- Because the target variable takes only 0/1 value, you could use a classification algorithm. \n",
    "\n",
    "Before jumping to modeling, it is always a good practice to look at feature distribution, correlations, etc.\n",
    "- This will give an idea of any non-linearity/patterns in the data.\n",
    "    - Linear models: Add power/exponential/interaction features\n",
    "    - Try a non-linear model\n",
    "- Data imbalance \n",
    "    - Choose metrics that will not give biased model performance (accuracy. vs AUC)\n",
    "    - Use weighted/custom loss functions\n",
    "- Missing data\n",
    "    - Do imputation based on simple statistics - mean, median, mode (numerical variables) frequent class (categorical variables)\n",
    "    - Clustering based imputation (KNNs to predict column value)\n",
    "    - Drop column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data exploration\n",
    "\n",
    "#### Check class delay vs. no delay\n",
    "\n",
    "**Hint**: Use a `groupby` plot ([documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html)) with a `bar` plot ([documentation](https://matplotlib.org/tutorials/introductory/pyplot.html)) to plot the frequency vs. distribution of the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(data.groupby(<CODE>).size()/len(data) ).plot(kind='bar')# Enter your code here\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of classes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: What can you deduce from the bar plot about the ratio of delay vs. no delay?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**: \n",
    "\n",
    "- Which months have the most delays?\n",
    "- What time of the day has the most delays?\n",
    "- What day of the week has the most delays?\n",
    "- Which airline has the most delays?\n",
    "- Which origin and destination airports have the most delays?\n",
    "- Is flight distance a factor in the delays?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_columns = ['Month', 'DepHourofDay', 'DayOfWeek', 'Reporting_Airline', 'Origin', 'Dest']\n",
    "fig, axes = plt.subplots(3, 2, figsize=(20,20), squeeze=False)\n",
    "# fig.autofmt_xdate(rotation=90)\n",
    "\n",
    "for idx, column in enumerate(viz_columns):\n",
    "    ax = axes[idx//2, idx%2]\n",
    "    temp = data.groupby(column)['is_delay'].value_counts(normalize=True).rename('percentage').\\\n",
    "    mul(100).reset_index().sort_values(column)\n",
    "    sns.barplot(x=column, y=\"percentage\", hue=\"is_delay\", data=temp, ax=ax)\n",
    "    plt.ylabel('% delay/no-delay')\n",
    "    \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot( x=\"is_delay\", y=\"Distance\", data=data, fit_reg=False, hue='is_delay', legend=False)\n",
    "plt.legend(loc='center')\n",
    "plt.xlabel('is_delay')\n",
    "plt.ylabel('Distance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your answers here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features\n",
    "\n",
    "Look at all the columns and what their specific types are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering the required columns:\n",
    "- Date is redundant, because you have Year, Quarter, Month, DayofMonth, and DayOfWeek to describe the date.\n",
    "- Use Origin and Dest codes instead of OriginState and DestState.\n",
    "- Because you are just classifying whether the flight is delayed or not, you don't need TotalDelayMinutes, DepDelayMinutes, and ArrDelayMinutes.\n",
    "\n",
    "Treat DepHourofDay as a categorical variable because it doesn't have any quantitative relation with the target.\n",
    "- If you had to do a one-hot encoding of it, it would result in 23 more columns.\n",
    "- Other alternatives to handling categorical variables include hash encoding, regularized mean encoding, and bucketizing the values, among others.\n",
    "- Just split into buckets here.\n",
    "\n",
    "**Hint**: To change a column type to category, use the `astype` function ([documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.astype.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_orig = data.copy()\n",
    "data = data[[ 'Quarter', 'Month', 'DayofMonth', 'DayOfWeek', \n",
    "       'Reporting_Airline', 'Origin', 'Dest','Distance','DepHourofDay', 'is_delay']]\n",
    "categorical_columns  = ['Quarter', 'Month', 'DayofMonth', 'DayOfWeek', \n",
    "       'Reporting_Airline', 'Origin', 'Dest', 'DepHourofDay', 'is_delay']\n",
    "for c in categorical_columns:\n",
    "    data[c] = data[c].astype('category')# Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use one-hot encoding, use the Pandas `get_dummies` function for the categorical columns that you selected above. Then, you can concatenate those generated features to your original dataset using the Pandas `concat` function. For encoding categorical variables, you can also use *dummy encoding* by using a keyword `drop_first=True`. For more information on dummy encoding, see https://en.wikiversity.org/wiki/Dummy_variable_(statistics).\n",
    "\n",
    "For example:\n",
    "```\n",
    "pd.get_dummies(df[['column1','columns2']], drop_first=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dummies = pd.get_dummies(<CODE>, drop_first=True) # Enter your code here\n",
    "data = pd.concat([<CODE>, <CODE>], axis = 1)\n",
    "categorical_columns.remove('is_delay')\n",
    "data.drop(categorical_columns,axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the length of the dataset and the new columnms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sample Answer:** \n",
    "```\n",
    "Index(['Distance', 'is_delay', 'Quarter_2', 'Quarter_3', 'Quarter_4',\n",
    "       'Month_2', 'Month_3', 'Month_4', 'Month_5', 'Month_6', 'Month_7',\n",
    "       'Month_8', 'Month_9', 'Month_10', 'Month_11', 'Month_12',\n",
    "       'DayofMonth_2', 'DayofMonth_3', 'DayofMonth_4', 'DayofMonth_5',\n",
    "       'DayofMonth_6', 'DayofMonth_7', 'DayofMonth_8', 'DayofMonth_9',\n",
    "       'DayofMonth_10', 'DayofMonth_11', 'DayofMonth_12', 'DayofMonth_13',\n",
    "       'DayofMonth_14', 'DayofMonth_15', 'DayofMonth_16', 'DayofMonth_17',\n",
    "       'DayofMonth_18', 'DayofMonth_19', 'DayofMonth_20', 'DayofMonth_21',\n",
    "       'DayofMonth_22', 'DayofMonth_23', 'DayofMonth_24', 'DayofMonth_25',\n",
    "       'DayofMonth_26', 'DayofMonth_27', 'DayofMonth_28', 'DayofMonth_29',\n",
    "       'DayofMonth_30', 'DayofMonth_31', 'DayOfWeek_2', 'DayOfWeek_3',\n",
    "       'DayOfWeek_4', 'DayOfWeek_5', 'DayOfWeek_6', 'DayOfWeek_7',\n",
    "       'Reporting_Airline_DL', 'Reporting_Airline_OO', 'Reporting_Airline_UA',\n",
    "       'Reporting_Airline_WN', 'Origin_CLT', 'Origin_DEN', 'Origin_DFW',\n",
    "       'Origin_IAH', 'Origin_LAX', 'Origin_ORD', 'Origin_PHX', 'Origin_SFO',\n",
    "       'Dest_CLT', 'Dest_DEN', 'Dest_DFW', 'Dest_IAH', 'Dest_LAX', 'Dest_ORD',\n",
    "       'Dest_PHX', 'Dest_SFO'],\n",
    "      dtype='object')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you are ready to do model training. Before splitting the data, rename the column `is_delay` to `target`.\n",
    "\n",
    "**Hint**: You can use the Pandas `rename` function ([documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.rename.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rename(columns = {<CODE>:<CODE>}, inplace=True )# Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\"> End of Lab 2 </span>\n",
    "\n",
    "Save the project file to your local computer. Follow these steps:\n",
    "\n",
    "1. At the top of the page, click the **File** menu. \n",
    "\n",
    "1. Select **Download as**, and click **Notebook(.ipynb)**.  \n",
    "\n",
    "This downloads the current notebook to the default download folder on your computer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Model training and evaluation\n",
    "\n",
    "There are some preliminary steps that you must include when converting the dataset from a dataframe to a format that a machine learning algorithm can use. For Amazon SageMaker, here are the steps you need to take:\n",
    "\n",
    "1. Split the data into `train_data`, `validation_data`, and `test_data` using `sklearn.model_selection.train_test_split`.  \n",
    "2. Convert the dataset to an appropriate file format that the Amazon SageMaker training job can use. This can be either a CSV file or record protobuf. For more information, see [Common Data Formats for Training](https://docs.aws.amazon.com/sagemaker/latest/dg/cdf-training.html).  \n",
    "3. Upload the data to your Amazon S3 bucket. If you have not created one before, see [Create a Bucket](https://docs.aws.amazon.com/AmazonS3/latest/gsg/CreatingABucket.html).  \n",
    "\n",
    "Use the following cells to complete these steps. Insert and delete cells where needed.\n",
    "\n",
    "#### <span style=\"color: blue;\">Project presentation: Take note of the key decisions you've made in this phase in your project presentation.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def create_training_sets(data):\n",
    "    \"\"\"\n",
    "    Convert data frame to train, validation and test\n",
    "    params:\n",
    "        data: The dataframe with the dataset to be split\n",
    "    Returns:\n",
    "        train_features: Training feature dataset\n",
    "        test_features: Test feature dataset \n",
    "        train_labels: Labels for the training dataset\n",
    "        test_labels: Labels for the test dataset\n",
    "        val_features: Validation feature dataset\n",
    "        val_labels: Labels for the validation dataset\n",
    "    \"\"\"\n",
    "    # Extract the target variable from the dataframe and convert the type to float32\n",
    "    ys = np.array(<CODE>).astype(\"float32\") # Enter your code here\n",
    "    \n",
    "    # Drop all the unwanted columns including the target column\n",
    "    drop_list = # Enter your code here\n",
    "    \n",
    "    # Drop the columns from the drop_list and convert the data into a NumPy array of type float32\n",
    "    xs = np.array(data.drop(<CODE>, axis=1)).astype(\"float32\")# Enter your code here\n",
    "    \n",
    "    np.random.seed(0)\n",
    "\n",
    "    # Use the sklearn function train_test_split to split the dataset in the ratio train 80% and test 20%\n",
    "    # Example: train_test_split(x, y, test_size=0.3)\n",
    "    train_features, test_features, train_labels, test_labels = # Enter your code here\n",
    "    \n",
    "    # Use the sklearn function again to split the test dataset into 50% validation and 50% test\n",
    "    val_features, test_features, val_labels, test_labels = # Enter your code here\n",
    "    \n",
    "    \n",
    "    return train_features, test_features, train_labels, test_labels, val_features, val_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the function to create your datasets\n",
    "train_features, test_features, train_labels, test_labels, val_features, val_labels = create_training_sets(data)\n",
    "\n",
    "print(f\"Length of train_features is: {<CODE>}\")\n",
    "print(f\"Length of train_labels is: {<CODE>}\")\n",
    "print(f\"Length of val_features is: {<CODE>}\")\n",
    "print(f\"Length of val_labels is: {<CODE>}\")\n",
    "print(f\"Length of test_features is: {<CODE>}\")\n",
    "print(f\"Length of test_labels is: {<CODE>}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sample answer**\n",
    "```\n",
    "Length of train_features is: (1308472, 71)\n",
    "Length of train_labels is: (1308472,)\n",
    "Length of val_features is: (163559, 71)\n",
    "Length of val_labels is: (163559,)\n",
    "Length of test_features is: (163559, 71)\n",
    "Length of test_labels is: (163559,)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.predictor import csv_serializer\n",
    "from sagemaker.amazon.amazon_estimator import RecordSet\n",
    "import boto3\n",
    "\n",
    "num_classes = # Enter your code here\n",
    "\n",
    "# Instantiate the LinearLearner estimator object with 1 ml.m4.xlarge\n",
    "classifier_estimator = sagemaker.LinearLearner(role=sagemaker.get_execution_role(),\n",
    "                                               train_instance_count=<CODE>,\n",
    "                                               train_instance_type=<CODE>,\n",
    "                                               predictor_type=<CODE>,\n",
    "                                              binary_classifier_model_selection_criteria=<CODE>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Code\n",
    "```\n",
    "num_classes = len(pd.unique(train_labels))\n",
    "classifier_estimator = sagemaker.LinearLearner(role=sagemaker.get_execution_role(),\n",
    "                                              train_instance_count=1,\n",
    "                                              train_instance_type='ml.m4.xlarge',\n",
    "                                              predictor_type='binary_classifier',\n",
    "                                             binary_classifier_model_selection_criteria = 'cross_entropy_loss')\n",
    "                                              \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear learner accepts training data in protobuf or CSV content types, and accepts inference requests in protobuf, CSV, or JSON content types. Training data has features and ground-truth labels, while the data in an inference request has only features.\n",
    "\n",
    "In a production pipeline, it is recommended to convert the data to the Amazon SageMaker protobuf format and store it in Amazon S3. However, to get up and running quickly, AWS provides the convenient method `record_set` for converting and uploading when the dataset is small enough to fit in local memory. It accepts NumPy arrays like the ones you already have, so let's use it here. The `RecordSet` object will keep track of the temporary Amazon S3 location of your data. Use the `estimator.record_set` function to create train, validation, and test records. Then, use the `estimator.fit` function to start your training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create train, val, test records\n",
    "train_records = linear.record_set(<CODE>,<CODE>, channel='train')# Enter your code here\n",
    "val_records = linear.record_set(<CODE>,<CODE>, channel='validation')# Enter your code here\n",
    "test_records = linear.record_set(<CODE>,<CODE>, channel='test')# Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, train your model on the dataset that you just uplaoded.\n",
    "\n",
    "### Sample code\n",
    "```\n",
    "linear.fit([train_records,val_records,test_records])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fit the classifier\n",
    "# Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation\n",
    "In this section, you'll evaluate your trained model. First, use the `estimator.deploy` function with `initial_instance_count= 1` and `instance_type= 'ml.m4.xlarge'` to deploy your model on Amazon SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Deloy an endpoint for batch prediction\n",
    "classifier_predictor = classifier_estimator.deploy(initial_instance_count=<CODE>, \n",
    "                                                   instance_type=<CODE>) # Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once your endpoint is 'InService', evaluate how your model performs on the test set. Use the `predict_batches` function to predict the metrics on your test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "def predict_batches(predictor, features, labels):\n",
    "    \"\"\"\n",
    "    Return evaluation results\n",
    "    predictor : Predictor object of model\n",
    "    features: Input features to model\n",
    "    label: Ground truth target values\n",
    "    \"\"\"\n",
    "    prediction_batches = [predictor.predict(batch) for batch in np.array_split(features, 100)]\n",
    "\n",
    "    # Parse protobuf responses to extract predicted labels\n",
    "    extract_label = lambda x: x.label['predicted_label'].float32_tensor.values\n",
    "    preds = np.concatenate([np.array([extract_label(x) for x in batch]) for batch in prediction_batches])\n",
    "    preds = preds.reshape((-1,))\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = (preds == labels).sum() / labels.shape[0]\n",
    "    print(f'Accuracy: {accuracy}')\n",
    "    \n",
    "    auc = roc_auc_score(labels, preds)\n",
    "    print(f'AUC     : {auc}')\n",
    "    \n",
    "    precision, recall, f1_score, _ = precision_recall_fscore_support(labels, preds, average = 'binary')\n",
    "    print(f'Precision: {precision}')\n",
    "    print(f'Recall: {recall}')\n",
    "    print(f'F1_score: {f1_score}')\n",
    "    \n",
    "    confusion_matrix = pd.crosstab(index=labels, columns=np.round(preds), rownames=['True'], colnames=['predictions']).astype(int)\n",
    "    plt.figure(figsize = (5,5))\n",
    "    sns.heatmap(confusion_matrix, annot=True, fmt='.2f', cmap=\"YlGnBu\").set_title('Confusion Matrix') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view the confusion matrix, run the `predict_batches` function on your test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key questions to consider:\n",
    "1. How does your model's performance on the test set compare to the training set? What can you deduce from this comparison? \n",
    "\n",
    "2. Are there obvious differences between the outcomes of metrics like accuracy, precision, and recall? If so, why might you be seeing those differences? \n",
    "\n",
    "3. Given your business situation and goals, which metric(s) is most important for you to consider here? Why?\n",
    "\n",
    "4. Is the outcome for the metric(s) you consider most important sufficient for what you need from a business standpoint? If not, what are some things you might change in your next iteration (in the feature engineering section, which is coming up next)? \n",
    "\n",
    "Use the cells below to answer these and other questions. Insert and delete cells where needed.\n",
    "\n",
    "#### <span style=\"color: blue;\">Project presentation: Record answers to these and other similar questions you might answer in this section in your project presentations. Record key details and decisions you've made in your project presentations.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Question**: What can you summarize from the confusion matrix?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\"> End of Lab 3 </span>\n",
    "\n",
    "Save the project file to your local computer. Follow these steps:\n",
    "\n",
    "1. At the top of the page, click the **File** menu. \n",
    "\n",
    "1. Select **Download as**, and click **Notebook(.ipynb)**.  \n",
    "\n",
    "This downloads the current notebook to the default download folder on your computer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iteration II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Feature engineering\n",
    "\n",
    "You've now gone through one iteration of training and evaluating your model. Given that the outcome you reached for your model the first time probably wasn't sufficient for solving your business problem, what are some things you could change about your data to possibly improve model performance?\n",
    "\n",
    "### Key questions to consider:\n",
    "1. How might the balance of your two main classes (delay and no delay) impact model performance?\n",
    "2. Do you have any features that are correlated?\n",
    "3. Are there feature reduction techniques you could perform at this stage that might have a positive impact on model performance? \n",
    "4. Can you think of adding some more data/datasets?\n",
    "4. After performing some feature engineering, how does your model performance compare to the first iteration?\n",
    "\n",
    "Use the cells below to perform specific feature engineering techniques (per the questions above) that you think could improve your model performance. Insert and delete cells where needed.\n",
    "\n",
    "#### <span style=\"color: blue;\">Project presentation: Record key decisions and methods you use in this section in your project presentations, as well as any new performance metrics you obtain after evaluating your model again.</span>\n",
    "\n",
    "Before you start, think about why the precision and recall are around 80% while the accuracy is 99%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add more features\n",
    "\n",
    "1. Holidays\n",
    "2. Weather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the list of holidays from 2014 to 2018 is known, you can create an indicator variable **is_holiday** to mark these.\n",
    "The hypothesis is that airplane delays could be higher during holidays compared to the rest of the days. Add a boolean variable `is_holiday` that includes the holidays for the years 2014-2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: http://www.calendarpedia.com/holidays/federal-holidays-2014.html\n",
    "\n",
    "holidays_14 = ['2014-01-01',  '2014-01-20', '2014-02-17', '2014-05-26', '2014-07-04', '2014-09-01', '2014-10-13', '2014-11-11', '2014-11-27', '2014-12-25' ] \n",
    "holidays_15 = ['2015-01-01',  '2015-01-19', '2015-02-16', '2015-05-25', '2015-06-03', '2015-07-04', '2015-09-07', '2015-10-12', '2015-11-11', '2015-11-26', '2015-12-25'] \n",
    "holidays_16 = ['2016-01-01',  '2016-01-18', '2016-02-15', '2016-05-30', '2016-07-04', '2016-09-05', '2016-10-10', '2016-11-11', '2016-11-24', '2016-12-25', '2016-12-26']\n",
    "holidays_17 = ['2017-01-02', '2017-01-16', '2017-02-20', '2017-05-29' , '2017-07-04', '2017-09-04' ,'2017-10-09', '2017-11-10', '2017-11-23', '2017-12-25']\n",
    "holidays_18 = ['2018-01-01', '2018-01-15', '2018-02-19', '2018-05-28' , '2018-07-04', '2018-09-03' ,'2018-10-08', '2018-11-12','2018-11-22', '2018-12-25']\n",
    "# holidays_19 = ['2019-01-01', '2019-01-21', '2019-02-18', '2019-05-27' , '2019-07-04', '2019-09-02' ,'2019-10-14', '2019-11-11','2019-11-28', '2019-12-25']\n",
    "holidays = holidays_14+ holidays_15+ holidays_16 + holidays_17+ holidays_18\n",
    "\n",
    "### Add indicator variable for holidays\n",
    "data_orig['is_holiday'] = # Enter your code here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weather data was fetched from https://www.ncei.noaa.gov/access/services/data/v1?dataset=daily-summaries&stations=USW00023174,USW00012960,USW00003017,USW00094846,USW00013874,USW00023234,USW00003927,USW00023183,USW00013881&dataTypes=AWND,PRCP,SNOW,SNWD,TAVG,TMIN,TMAX&startDate=2014-01-01&endDate=2018-12-31.\n",
    "<br>\n",
    "\n",
    "This dataset has information on wind speed, precipitation, snow, and temperature for cities by their airport codes.\n",
    "\n",
    "**Question**: Could bad weather due to rains, heavy winds, or snow lead to airplane delay? Let's check!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "aws s3 cp s3://aws-tc-largeobjects/ILT-TF-200-MLDWTS/flight_delay_project/daily-summaries.csv /home/ec2-user/SageMaker/project/data/\n",
    "#wget 'https://www.ncei.noaa.gov/access/services/data/v1?dataset=daily-summaries&stations=USW00023174,USW00012960,USW00003017,USW00094846,USW00013874,USW00023234,USW00003927,USW00023183,USW00013881&dataTypes=AWND,PRCP,SNOW,SNWD,TAVG,TMIN,TMAX&startDate=2014-01-01&endDate=2018-12-31' -O /home/ec2-user/SageMaker/project/data/daily-summaries.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import weather data prepared for the airport codes in our dataset. Use the stations and airports below for the analysis, and create a new column called `airport` that maps the weather station to the airport name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = pd.read_csv(<CODE>) # Enter your code here to read 'daily-summaries.csv' file\n",
    "station = ['USW00023174','USW00012960','USW00003017','USW00094846',\n",
    "           'USW00013874','USW00023234','USW00003927','USW00023183','USW00013881'] \n",
    "airports = ['LAX', 'IAH', 'DEN', 'ORD', 'ATL', 'SFO', 'DFW', 'PHX', 'CLT']\n",
    "\n",
    "### Map weather stations to airport code\n",
    "station_map = # Enter your code here \n",
    "weather['airport'] = # Enter your code here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create another column called `MONTH` from the `DATE` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weather['MONTH'] = weather[<CODE>].apply(lambda x: x.split('-')[1])# Enter your code here \n",
    "weather.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample output\n",
    "```\n",
    "  STATION     DATE      AWND PRCP SNOW SNWD TAVG TMAX  TMIN airport MONTH\n",
    "0 USW00023174 2014-01-01 16   0   NaN  NaN 131.0 178.0 78.0  LAX    01\n",
    "1 USW00023174 2014-01-02 22   0   NaN  NaN 159.0 256.0 100.0 LAX    01\n",
    "2 USW00023174 2014-01-03 17   0   NaN  NaN 140.0 178.0 83.0  LAX    01\n",
    "3 USW00023174 2014-01-04 18   0   NaN  NaN 136.0 183.0 100.0 LAX    01\n",
    "4 USW00023174 2014-01-05 18   0   NaN  NaN 151.0 244.0 83.0  LAX    01\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze and handle the `SNOW` and `SNWD` columns for missing values using `fillna()`. Use the `isna()` function to check the missing values for all the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weather.SNOW.fillna(<CODE>, inplace=True)# Enter your code here\n",
    "weather.SNWD.fillna(<CODE>, inplace=True)# Enter your code here\n",
    "weather.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Print the index of the rows that have missing values for TAVG, TMAX, TMIN.\n",
    "\n",
    "**Hint**: Use the `isna()` function to find the rows that are missing, and then use the list on the idx variable to get the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.array([i for i in range(len(weather))])\n",
    "TAVG_idx = # Enter your code here \n",
    "TMAX_idx = # Enter your code here \n",
    "TMIN_idx = # Enter your code here \n",
    "TAVG_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample output\n",
    "\n",
    "```\n",
    "array([ 3956,  3957,  3958,  3959,  3960,  3961,  3962,  3963,  3964,\n",
    "        3965,  3966,  3967,  3968,  3969,  3970,  3971,  3972,  3973,\n",
    "        3974,  3975,  3976,  3977,  3978,  3979,  3980,  3981,  3982,\n",
    "        3983,  3984,  3985,  4017,  4018,  4019,  4020,  4021,  4022,\n",
    "        4023,  4024,  4025,  4026,  4027,  4028,  4029,  4030,  4031,\n",
    "        4032,  4033,  4034,  4035,  4036,  4037,  4038,  4039,  4040,\n",
    "        4041,  4042,  4043,  4044,  4045,  4046,  4047, 13420])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can replace the missing TAVG, TMAX, and TMIN with the average value for a particular station/airport. Because the consecutive rows of TAVG_idx are missing, replacing with a previous value would not be possible. Instead, replace it with the mean. Use the `groupby` function to aggregate the variables with a mean value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_impute = weather.groupby([<CODE>]).agg({'TAVG':'mean','TMAX':'mean', 'TMIN':'mean' }).reset_index()# Enter your code here\n",
    "weather_impute.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the mean data with the weather data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### get the yesterday's data\n",
    "weather = pd.merge(weather, weather_impute,  how='left', left_on=['MONTH','STATION'], right_on = ['MONTH','STATION'])\\\n",
    ".rename(columns = {'TAVG_y':'TAVG_AVG',\n",
    "                   'TMAX_y':'TMAX_AVG', \n",
    "                   'TMIN_y':'TMIN_AVG',\n",
    "                   'TAVG_x':'TAVG',\n",
    "                   'TMAX_x':'TMAX', \n",
    "                   'TMIN_x':'TMIN'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for missing values again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.TAVG[TAVG_idx] = weather.TAVG_AVG[TAVG_idx]\n",
    "weather.TMAX[TMAX_idx] = weather.TMAX_AVG[TMAX_idx]\n",
    "weather.TMIN[TMIN_idx] = weather.TMIN_AVG[TMIN_idx]\n",
    "weather.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop `STATION,MONTH,TAVG_AVG,TMAX_AVG,TMIN_AVG,TMAX,TMIN,SNWD` from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.drop(columns=['STATION','MONTH','TAVG_AVG', 'TMAX_AVG', 'TMIN_AVG', 'TMAX' ,'TMIN', 'SNWD'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the origin and destination weather conditions to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add origin weather conditions\n",
    "data_orig = pd.merge(data_orig, weather,  how='left', left_on=['FlightDate','Origin'], right_on = ['DATE','airport'])\\\n",
    ".rename(columns = {'AWND':'AWND_O','PRCP':'PRCP_O', 'TAVG':'TAVG_O', 'SNOW': 'SNOW_O'})\\\n",
    ".drop(columns=['DATE','airport'])\n",
    "\n",
    "### Add destination weather conditions\n",
    "data_orig = pd.merge(data_orig, weather,  how='left', left_on=['FlightDate','Dest'], right_on = ['DATE','airport'])\\\n",
    ".rename(columns = {'AWND':'AWND_D','PRCP':'PRCP_D', 'TAVG':'TAVG_D', 'SNOW': 'SNOW_D'})\\\n",
    ".drop(columns=['DATE','airport'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: It is always a good practice to check nulls/NAs after joins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(data.isna().any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_orig.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the categorical data into numerical data using one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_orig.copy()\n",
    "data = data[['Year', 'Quarter', 'Month', 'DayofMonth', 'DayOfWeek', \n",
    "       'Reporting_Airline', 'Origin', 'Dest','Distance','DepHourofDay', 'is_delay', 'is_holiday', 'AWND_O', 'PRCP_O',\n",
    "       'TAVG_O', 'AWND_D', 'PRCP_D', 'TAVG_D', 'SNOW_O', 'SNOW_D']]\n",
    "\n",
    "\n",
    "categorical_columns  = ['Year', 'Quarter', 'Month', 'DayofMonth', 'DayOfWeek', \n",
    "       'Reporting_Airline', 'Origin', 'Dest', 'is_holiday','is_delay']\n",
    "for c in categorical_columns:\n",
    "    data[c] = data[c].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dummies = # Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample code\n",
    "\n",
    "```\n",
    "data_dummies = pd.get_dummies(data[['Year', 'Quarter', 'Month', 'DayofMonth', 'DayOfWeek', 'Reporting_Airline', 'Origin', 'Dest', 'is_holiday']], drop_first=True)\n",
    "data = pd.concat([data, data_dummies], axis = 1)\n",
    "categorical_columns.remove('is_delay')\n",
    "data.drop(categorical_columns,axis=1, inplace=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the new columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample output\n",
    "\n",
    "```\n",
    "Index(['Distance', 'DepHourofDay', 'is_delay', 'AWND_O', 'PRCP_O', 'TAVG_O',\n",
    "       'AWND_D', 'PRCP_D', 'TAVG_D', 'SNOW_O', 'SNOW_D', 'Year_2015',\n",
    "       'Year_2016', 'Year_2017', 'Year_2018', 'Quarter_2', 'Quarter_3',\n",
    "       'Quarter_4', 'Month_2', 'Month_3', 'Month_4', 'Month_5', 'Month_6',\n",
    "       'Month_7', 'Month_8', 'Month_9', 'Month_10', 'Month_11', 'Month_12',\n",
    "       'DayofMonth_2', 'DayofMonth_3', 'DayofMonth_4', 'DayofMonth_5',\n",
    "       'DayofMonth_6', 'DayofMonth_7', 'DayofMonth_8', 'DayofMonth_9',\n",
    "       'DayofMonth_10', 'DayofMonth_11', 'DayofMonth_12', 'DayofMonth_13',\n",
    "       'DayofMonth_14', 'DayofMonth_15', 'DayofMonth_16', 'DayofMonth_17',\n",
    "       'DayofMonth_18', 'DayofMonth_19', 'DayofMonth_20', 'DayofMonth_21',\n",
    "       'DayofMonth_22', 'DayofMonth_23', 'DayofMonth_24', 'DayofMonth_25',\n",
    "       'DayofMonth_26', 'DayofMonth_27', 'DayofMonth_28', 'DayofMonth_29',\n",
    "       'DayofMonth_30', 'DayofMonth_31', 'DayOfWeek_2', 'DayOfWeek_3',\n",
    "       'DayOfWeek_4', 'DayOfWeek_5', 'DayOfWeek_6', 'DayOfWeek_7',\n",
    "       'Reporting_Airline_DL', 'Reporting_Airline_OO', 'Reporting_Airline_UA',\n",
    "       'Reporting_Airline_WN', 'Origin_CLT', 'Origin_DEN', 'Origin_DFW',\n",
    "       'Origin_IAH', 'Origin_LAX', 'Origin_ORD', 'Origin_PHX', 'Origin_SFO',\n",
    "       'Dest_CLT', 'Dest_DEN', 'Dest_DFW', 'Dest_IAH', 'Dest_LAX', 'Dest_ORD',\n",
    "       'Dest_PHX', 'Dest_SFO', 'is_holiday_1'],\n",
    "      dtype='object')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename the `is_delay` column to `target` again. Use the same code as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rename(columns = {<CODE>:<CODE>}, inplace=True )# Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the training sets again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New baseline classifier\n",
    "\n",
    "Now, see if these new features add any predictive power to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the LinearLearner estimator object\n",
    "num_classes = # Enter your code here\n",
    "classifier_estimator = # Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample code\n",
    "\n",
    "```\n",
    "num_classes = len(pd.unique(train_labels)) \n",
    "classifier_estimator = sagemaker.LinearLearner(role=sagemaker.get_execution_role(),\n",
    "                                               train_instance_count=1,\n",
    "                                               train_instance_type='ml.m4.xlarge',\n",
    "                                               predictor_type='binary_classifier',\n",
    "                                              binary_classifier_model_selection_criteria = 'cross_entropy_loss')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_records = classifier_estimator.record_set(train_features, train_labels, channel='train')\n",
    "val_records = classifier_estimator.record_set(val_features, val_labels, channel='validation')\n",
    "test_records = classifier_estimator.record_set(test_features, test_labels, channel='test')\n",
    "\n",
    "classifier_estimator.fit([train_records, val_records, test_records])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_predictor = # Enter your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_batches(classifier_predictor, test_features, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear model shows only a little improvement in performance. Try a tree-based ensemble model called XGBoost with Amazon SageMaker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try the XGBoost model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the steps you need to take:  \n",
    "\n",
    "1. Use the training set variables and save them as CSV files: train.csv, validation.csv and test.csv  \n",
    "2. Store the bucket name in the variable. The Amazon S3 bucket name is provided to the left of the lab instructions.  \n",
    "a. `bucket = <LabBucketName>`  \n",
    "b. `prefix = 'sagemaker/xgboost'`  \n",
    "3. Use Boto3 to upload the model to the bucket.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, validation_data, test_data = np.split(data.sample(frac=1, random_state=1729), [int(0.7 * len(data)), int(0.9*len(data))])  \n",
    "\n",
    "pd.concat([train_data['target'], train_data.drop(['target'], axis=1)], axis=1).to_csv('train.csv', index=False, header=False)\n",
    "pd.concat([validation_data['target'], validation_data.drop(['target'], axis=1)], axis=1).to_csv('validation.csv', index=False, header=False)\n",
    "pd.concat([test_data['target'], test_data.drop(['target'], axis=1)], axis=1).to_csv('test.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace **`<LabBucketName>`** with the resource name that was provided with your lab account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code here to define bucket and prefix\n",
    "bucket = <LabBucketName> # Enter your code here\n",
    "prefix = 'sagemaker/xgboost'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Upload data to S3\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train/train.csv')).upload_file('train.csv')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'validation/validation.csv')).upload_file('validation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "container = get_image_uri(boto3.Session().region_name, 'xgboost')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `sagemaker.s3_input` function to create a record_set for the training and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_input_train = sagemaker.s3_input(s3_data='s3://{}/{}/train'.format(bucket, prefix), content_type='csv')\n",
    "s3_input_validation = sagemaker.s3_input(s3_data='s3://{}/{}/validation/'.format(bucket, prefix), content_type='csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "\n",
    "xgb = sagemaker.estimator.Estimator(container,\n",
    "                                    role = sagemaker.get_execution_role(), \n",
    "                                    train_instance_count=1, \n",
    "                                    train_instance_type='ml.m4.xlarge',\n",
    "                                    output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
    "                                    sagemaker_session=sess)\n",
    "xgb.set_hyperparameters(max_depth=5,\n",
    "                        eta=0.2,\n",
    "                        gamma=4,\n",
    "                        min_child_weight=6,\n",
    "                        subsample=0.8,\n",
    "                        silent=0,\n",
    "                        objective='binary:logistic',\n",
    "                        eval_metric = \"auc\", \n",
    "                        num_round=100)\n",
    "\n",
    "xgb.fit({'train': s3_input_train, 'validation': s3_input_validation})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deploy the predictor for your new model and evaluate the model on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor = xgb.deploy(initial_instance_count = <CODE>, \n",
    "                           instance_type = <CODE>)# Enter your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.concat([test_data['target'], test_data.drop(['target'], axis=1)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor.content_type = 'text/csv'\n",
    "xgb_predictor.serializer = csv_serializer\n",
    "xgb_predictor.deserializer = None\n",
    "\n",
    "\n",
    "def predict(predictor , features, labels , prob_threshold = 0.5, rows=500):\n",
    "    \"\"\"\n",
    "    Return evaluation results\n",
    "    predictor : Predictor object of model\n",
    "    features: Input features to model\n",
    "    label: Ground truth target values\n",
    "    prob_threshold : Probability cut off to separate positive and negative class\n",
    "    \"\"\"\n",
    "    split_array = np.array_split(features, int(features.shape[0] / float(rows) + 1))\n",
    "    predictions = ''\n",
    "    for array in split_array:\n",
    "        predictions = ','.join([predictions, predictor.predict(array).decode('utf-8')])\n",
    "\n",
    "    preds = np.fromstring(predictions[1:], sep=',')\n",
    "    preds = preds.reshape((-1,))\n",
    "    predictions = np.where(preds > prob_threshold , 1, 0)\n",
    "    labels = labels.reshape((-1,))\n",
    "    \n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = (predictions == labels).sum() / labels.shape[0]\n",
    "    print(f'Accuracy: {accuracy}')\n",
    "    \n",
    "    auc = roc_auc_score(labels, preds)\n",
    "    print(f'AUC     : {auc}')\n",
    "    \n",
    "    precision, recall, f1_score, _ = precision_recall_fscore_support(labels, predictions, average = 'binary')\n",
    "    print(f'Precision: {precision}')\n",
    "    print(f'Recall: {recall}')\n",
    "    print(f'F1_score: {f1_score}')\n",
    "    \n",
    "    \n",
    "    confusion_matrix = pd.crosstab(index=labels, columns=np.round(preds), rownames=['True'], colnames=['predictions']).astype(int)\n",
    "    plt.figure(figsize = (5,5))\n",
    "    sns.heatmap(confusion_matrix, annot=True, fmt='.2f', cmap=\"YlGnBu\").set_title('Confusion Matrix') \n",
    "    return list(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict(xgb_predictor, test_data.as_matrix()[:, 1:] ,  test_data.iloc[:, 0:1].as_matrix(), prob_threshold = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try different thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict(xgb_predictor, test_data.as_matrix()[:, 1:] ,  test_data.iloc[:, 0:1].as_matrix(), prob_threshold = 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: What can you conclude from how the model did on the test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enter your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter optimization (HPO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner\n",
    "\n",
    "### You can spin up multiple instances to do hyperparameter optimization in parallel\n",
    "\n",
    "xgb = sagemaker.estimator.Estimator(container,\n",
    "                                    role=sagemaker.get_execution_role(), \n",
    "                                    train_instance_count= 2, # make sure you have limit set for these instances\n",
    "                                    train_instance_type='ml.m4.xlarge', \n",
    "                                    output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
    "                                    sagemaker_session=sess)\n",
    "\n",
    "xgb.set_hyperparameters(eval_metric='auc',\n",
    "                        objective='binary:logistic',\n",
    "                        num_round=100,\n",
    "                        rate_drop=0.3,\n",
    "                        tweedie_variance_power=1.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_ranges = {'eta': ContinuousParameter(0, 0.5),\n",
    "                        'min_child_weight': ContinuousParameter(3, 10),\n",
    "                         'num_round': IntegerParameter(10, 150),\n",
    "                        'alpha': ContinuousParameter(0, 2),\n",
    "                        'max_depth': IntegerParameter(10, 15)}\n",
    "\n",
    "objective_metric_name = 'validation:auc'\n",
    "\n",
    "tuner = HyperparameterTuner(xgb,\n",
    "                            objective_metric_name,\n",
    "                            hyperparameter_ranges,\n",
    "                            max_jobs=10,\n",
    "                            max_parallel_jobs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.fit({'train': s3_input_train, 'validation': s3_input_validation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boto3.client('sagemaker').describe_hyper_parameter_tuning_job(\n",
    "    HyperParameterTuningJobName=tuner.latest_tuning_job.job_name)['HyperParameterTuningJobStatus']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i class=\"fas fa-exclamation-triangle\" style=\"color:red\"></i> Wait until the training job is finished. It may take 25-30 minutes.\n",
    "\n",
    "**To monitor hyperparameter optimization jobs:**  \n",
    "\n",
    "1. In the AWS Management Console, on the **Services** menu, click **Amazon SageMaker**.  \n",
    "1. Click **Training** > **Hyperparameter tuning jobs**.  \n",
    "1. You can check the status of each hyperparametertuning job, its objective metric value, and its logs.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sage_client = boto3.Session().client('sagemaker')\n",
    "tuning_job_name = tuner.latest_tuning_job.job_name\n",
    "\n",
    "# Run this cell to check current status of hyperparameter tuning job\n",
    "tuning_job_result = sage_client.describe_hyper_parameter_tuning_job(HyperParameterTuningJobName=tuning_job_name)\n",
    "\n",
    "status = tuning_job_result['HyperParameterTuningJobStatus']\n",
    "if status != 'Completed':\n",
    "    print('Reminder: the tuning job has not been completed.')\n",
    "    \n",
    "job_count = tuning_job_result['TrainingJobStatusCounters']['Completed']\n",
    "print(\"%d training jobs have completed\" % job_count)\n",
    "    \n",
    "is_minimize = (tuning_job_result['HyperParameterTuningJobConfig']['HyperParameterTuningJobObjective']['Type'] != 'Maximize')\n",
    "objective_name = tuning_job_result['HyperParameterTuningJobConfig']['HyperParameterTuningJobObjective']['MetricName']\n",
    "\n",
    "\n",
    "from pprint import pprint\n",
    "if tuning_job_result.get('BestTrainingJob',None):\n",
    "    print(\"Best model found so far:\")\n",
    "    pprint(tuning_job_result['BestTrainingJob'])\n",
    "else:\n",
    "    print(\"No training jobs have reported results yet.\")\n",
    "    \n",
    "### Get the hyperparameter tuning job results in a dataframe\n",
    "tuner_df = sagemaker.HyperparameterTuningJobAnalytics(tuning_job_name).dataframe()\n",
    "\n",
    "if len(tuner_df) > 0:\n",
    "    df = tuner_df[tuner_df['FinalObjectiveValue'] > -float('inf')]\n",
    "    if len(df) > 0:\n",
    "        df = df.sort_values('FinalObjectiveValue', ascending=is_minimize)\n",
    "        print(\"Number of training jobs with valid objective: %d\" % len(df))\n",
    "        print({\"lowest\":min(df['FinalObjectiveValue']),\"highest\": max(df['FinalObjectiveValue'])})\n",
    "        pd.set_option('display.max_colwidth', -1)  # Don't truncate TrainingJobName        \n",
    "    else:\n",
    "        print(\"No training jobs have reported valid results yet.\")\n",
    "        \n",
    "tuner_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deploy the best model from the hyperparameter optimization training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor_hpo = # Enter your code here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor_hpo.content_type = 'text/csv'\n",
    "xgb_predictor_hpo.serializer = csv_serializer\n",
    "xgb_predictor_hpo.deserializer = None\n",
    "\n",
    "predictions = predict(xgb_predictor_hpo, test_data.as_matrix()[:, 1:] ,  test_data.iloc[:, 0:1].as_matrix(), prob_threshold = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Try different hyperparameters and hyperparameter ranges. Does this improve the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance\n",
    "\n",
    "The model files for all hyperparameter tuning jobs are saved by the training job name as in the table above in the folder: \"{bucket}/{prefix}/output/\". You can load the model and use it like a regular sklearn model object for interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_hpo_model_path = \"s3://\" + bucket + \"/sagemaker/xgboost/output/<Best model TrainingJobName>/output/model.tar.gz\"\n",
    "best_hpo_model_path = <path-to-your-model>\n",
    "### Download best_hpo_model_path to local\n",
    "!aws s3 cp {best_hpo_model_path} ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install xgboost\n",
    "!pip install xgboost=='0.90'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load picked model file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import tarfile\n",
    "import xgboost\n",
    "\n",
    "with open('model.tar.gz', 'rb') as f:\n",
    "    with tarfile.open(fileobj=f, mode='r') as tar_f:\n",
    "        with tar_f.extractfile('xgboost-model') as extracted_f:\n",
    "            xgbooster = pickle.load(extracted_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map column names to XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = list(data.columns)\n",
    "columns.remove('target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = xgbooster.get_fscore()\n",
    "feature_importance_col = {}\n",
    "\n",
    "for column, fname in zip(columns, xgbooster.feature_names):\n",
    "    try:\n",
    "         feature_importance_col[column] = feature_importance[fname]\n",
    "    except Exception:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sort by feature importance value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(feature_importance_col.items(), key=lambda kv: kv[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample output\n",
    "\n",
    "```\n",
    "[('AWND_O', 13851),\n",
    " ('AWND_D', 13452),\n",
    " ('DepHourofDay', 13344),\n",
    " ('TAVG_O', 13106),\n",
    " ('TAVG_D', 12800),\n",
    " ('Distance', 8478),\n",
    " ('PRCP_O', 4210),\n",
    " ('PRCP_D', 3916),\n",
    " ('Reporting_Airline_UA', 1791),\n",
    " ('Year_2016', 1290),\n",
    " ('Year_2015', 1285),\n",
    " ('Year_2018', 1157),\n",
    " ('Quarter_4', 1092),\n",
    " ('Year_2017', 1009),\n",
    " ('DayOfWeek_5', 838),\n",
    " ('DayOfWeek_4', 833),\n",
    " ('Quarter_2', 799),\n",
    " ('DayOfWeek_7', 783),\n",
    " ('Reporting_Airline_WN', 736),\n",
    " ('DayOfWeek_3', 718),\n",
    " ('Origin_ORD', 706),\n",
    " ('DayOfWeek_2', 685),\n",
    " ('Reporting_Airline_DL', 663),\n",
    " ('Dest_LAX', 627),\n",
    " ('DayOfWeek_6', 614),\n",
    " ('Reporting_Airline_OO', 596),\n",
    " ('Origin_LAX', 590),\n",
    " ('Month_11', 565),\n",
    " ('Month_5', 543),\n",
    " ('Dest_ORD', 539),\n",
    " ('SNOW_O', 506),\n",
    " ('Month_8', 497),\n",
    " ('Month_12', 495),\n",
    " ('Month_7', 484),\n",
    " ('Origin_DFW', 480),\n",
    " ('Quarter_3', 477),\n",
    " ('Dest_DFW', 462),\n",
    " ('Origin_DEN', 456),\n",
    " ('Origin_SFO', 438),\n",
    " ('Month_6', 437),\n",
    " ('Dest_SFO', 437),\n",
    " ('Month_10', 422),\n",
    " ('Month_9', 407),\n",
    " ('Month_4', 394),\n",
    " ('Dest_DEN', 379),\n",
    " ('SNOW_D', 375),\n",
    " ('Month_3', 369),\n",
    " ('Dest_IAH', 369),\n",
    " ('Origin_PHX', 347),\n",
    " ('Dest_PHX', 346),\n",
    " ('Origin_IAH', 342),\n",
    " ('DayofMonth_15', 324),\n",
    " ('DayofMonth_17', 322),\n",
    " ('DayofMonth_21', 322),\n",
    " ('Origin_CLT', 322),\n",
    " ('DayofMonth_20', 316),\n",
    " ('DayofMonth_9', 313),\n",
    " ('DayofMonth_19', 307),\n",
    " ('DayofMonth_26', 303),\n",
    " ('DayofMonth_7', 301),\n",
    " ('DayofMonth_4', 300),\n",
    " ('DayofMonth_2', 299),\n",
    " ('DayofMonth_27', 298),\n",
    " ('DayofMonth_3', 294),\n",
    " ('DayofMonth_28', 287),\n",
    " ('Month_2', 284),\n",
    " ('DayofMonth_5', 284),\n",
    " ('DayofMonth_12', 282),\n",
    " ('DayofMonth_11', 276),\n",
    " ('Dest_CLT', 276),\n",
    " ('DayofMonth_6', 275),\n",
    " ('DayofMonth_22', 274),\n",
    " ('DayofMonth_8', 273),\n",
    " ('DayofMonth_18', 273),\n",
    " ('DayofMonth_29', 267),\n",
    " ('DayofMonth_23', 266),\n",
    " ('DayofMonth_14', 263),\n",
    " ('DayofMonth_30', 260),\n",
    " ('DayofMonth_10', 259),\n",
    " ('DayofMonth_16', 257),\n",
    " ('DayofMonth_24', 255),\n",
    " ('DayofMonth_13', 245),\n",
    " ('DayofMonth_25', 237),\n",
    " ('is_holiday_1', 231),\n",
    " ('DayofMonth_31', 164)]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the feature importance above, you can see that DepHourofDay, Airwind, and Temperature at both source and destination are the major influencers in deciding the flight delay.\n",
    "\n",
    "This is one way to validate what is intuitive in terms of features and what the model actually learns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "You've now gone through at least a couple iterations of training and evaluating your model. It's time to wrap up this project and reflect on what you've learned and what types of steps you might take moving forward (assuming you had more time). Use the cell below to answer some of these and other relevant questions:\n",
    "\n",
    "1. Does your model performance meet your business goal? If not, what are some things you'd like to do differently if you had more time for tuning?\n",
    "2. To what extent did your model improve as you made changes to your dataset, features, and hyperparameters? What types of techniques did you employ throughout this project that you felt yielded the greatest improvements in your model?\n",
    "3. What were some of the biggest challenges you encountered throughout this project?\n",
    "4. Are there outstanding questions you have about aspects of the pipeline that just didn't make sense to you?\n",
    "5. What were the three most important things you learned about machine learning while completing this project?\n",
    "\n",
    "#### <span style=\"color: blue;\">Project presentation: Summarize your answers to these questions in your project presentation as well. Pull together all of your notes for your project presentation at this point and prepare to present your findings to the class.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
